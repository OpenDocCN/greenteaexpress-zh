["```py\nimport numpy as np\nfrom empiricaldist import Pmf\n\nhypos = np.arange(1, 1001)\nprior = Pmf(1, hypos) \n```", "```py\ndef update_train(pmf, data):\n  \"\"\"Update pmf based on new data.\"\"\"\n    hypos = pmf.qs\n    likelihood = 1 / hypos\n    impossible = (data > hypos)\n    likelihood[impossible] = 0\n    pmf *= likelihood\n    pmf.normalize() \n```", "```py\ndata = 60\nposterior = prior.copy()\nupdate_train(posterior, data) \n```", "```py\nfrom utils import decorate\n\nposterior.plot(label='Posterior after train 60', color='C4')\ndecorate(xlabel='Number of trains',\n         ylabel='PMF',\n         title='Posterior distribution') \n```", "```py\nposterior.max_prob() \n```", "```py\n60 \n```", "```py\nnp.sum(posterior.ps * posterior.qs) \n```", "```py\n333.41989326370776 \n```", "```py\nposterior.mean() \n```", "```py\n333.41989326370776 \n```", "```py\nimport pandas as pd\n\ndf = pd.DataFrame(columns=['Posterior mean'])\ndf.index.name = 'Upper bound'\n\nfor high in [500, 1000, 2000]:\n    hypos = np.arange(1, high+1)\n    pmf = Pmf(1, hypos)\n    update_train(pmf, data=60)\n    df.loc[high] = pmf.mean()\n\ndf \n```", "```py\ndf = pd.DataFrame(columns=['Posterior mean'])\ndf.index.name = 'Upper bound'\n\ndataset = [30, 60, 90]\n\nfor high in [500, 1000, 2000]:\n    hypos = np.arange(1, high+1)\n    pmf = Pmf(1, hypos)\n    for data in dataset:\n        update_train(pmf, data)\n    df.loc[high] = pmf.mean()\n\ndf \n```", "```py\nalpha = 1.0\nps = hypos**(-alpha)\npower = Pmf(ps, hypos, name='power law')\npower.normalize() \n```", "```py\n8.178368103610282 \n```", "```py\nhypos = np.arange(1, 1001)\nuniform = Pmf(1, hypos, name='uniform')\nuniform.normalize() \n```", "```py\n1000 \n```", "```py\nuniform.plot(color='C4')\npower.plot(color='C1')\n\ndecorate(xlabel='Number of trains',\n         ylabel='PMF',\n         title='Prior distributions') \n```", "```py\ndataset = [60]\nupdate_train(uniform, dataset)\nupdate_train(power, dataset) \n```", "```py\nuniform.plot(color='C4')\npower.plot(color='C1')\n\ndecorate(xlabel='Number of trains',\n         ylabel='PMF',\n         title='Posterior distributions') \n```", "```py\ndf = pd.DataFrame(columns=['Posterior mean'])\ndf.index.name = 'Upper bound'\n\nalpha = 1.0\ndataset = [30, 60, 90]\n\nfor high in [500, 1000, 2000]:\n    hypos = np.arange(1, high+1)\n    ps = hypos**(-alpha)\n    power = Pmf(ps, hypos)\n    for data in dataset:\n        update_train(power, data)\n    df.loc[high] = power.mean()\n\ndf \n```", "```py\npower.prob_le(100) \n```", "```py\n0.2937469222495771 \n```", "```py\ndef quantile(pmf, prob):\n  \"\"\"Compute a quantile with the given prob.\"\"\"\n    total = 0\n    for q, p in pmf.items():\n        total += p\n        if total >= prob:\n            return q\n    return np.nan \n```", "```py\nquantile(power, 0.5) \n```", "```py\n113 \n```", "```py\npower.quantile([0.05, 0.95]) \n```", "```py\narray([ 91., 243.]) \n```", "```py\npower.credible_interval(0.9) \n```", "```py\narray([ 91., 243.]) \n```", "```py\n# Solution\n\n# I'll use a uniform prior from 1 to 2000\n# (we'll see that the probability is small that there are\n# more than 2000 people in the room)\n\nhypos = np.arange(1, 2000, 10)\nprior = Pmf(1, hypos)\nprior.normalize() \n```", "```py\n200 \n```", "```py\n# Solution\n\n# We can use the binomial distribution to compute the probability\n# of the data for each hypothetical audience size\n\nfrom scipy.stats import binom\n\nlikelihood1 = binom.pmf(2, hypos, 1/365)\nlikelihood2 = binom.pmf(1, hypos, 1/365)\nlikelihood3 = binom.pmf(0, hypos, 1/365) \n```", "```py\n# Solution\n\n# Here's the update\n\nposterior = prior * likelihood1 * likelihood2 * likelihood3\nposterior.normalize() \n```", "```py\n0.006758799800451805 \n```", "```py\n# Solution\n\n# And here's the posterior distribution\n\nposterior.plot(color='C4', label='posterior')\ndecorate(xlabel='Number of people in the audience',\n         ylabel='PMF') \n```", "```py\n# Solution\n\n# If we have to guess the audience size,\n# we might use the posterior mean\n\nposterior.mean() \n```", "```py\n486.2255161687084 \n```", "```py\n# Solution\n\n# And we can use prob_gt to compute the probability\n# of exceeding the capacity of the room.\n\n# It's about 1%, which may or may not satisfy the fire marshal\n\nposterior.prob_gt(1200) \n```", "```py\n0.011543092507699223 \n```", "```py\n# Solution\n\nhypos = np.arange(4, 11)\nprior = Pmf(1, hypos) \n```", "```py\n# Solution\n\n# The probability that the second rabbit is the same as the first is 1/N\n# The probability that the third rabbit is different is (N-1)/N\n\nN = hypos\nlikelihood = (N-1) / N**2 \n```", "```py\n# Solution\n\nposterior = prior * likelihood\nposterior.normalize()\n\nposterior.bar(alpha=0.7)\ndecorate(xlabel='Number of rabbits',\n         ylabel='PMF',\n         title='The Rabbit Problem') \n```", "```py\n# Solution\n\n# Here's the prior distribution of sentences\n\nhypos = np.arange(1, 4)\nprior = Pmf(1/3, hypos)\nprior \n```", "```py\n# Solution\n\n# If you visit a prison at a random point in time,\n# the probability of observing any given prisoner\n# is proportional to the duration of their sentence.\n\nlikelihood = hypos\nposterior = prior * likelihood\nposterior.normalize()\nposterior \n```", "```py\n# Solution\n\n# The mean of the posterior is the average sentence.\n# We can divide by 2 to get the average remaining sentence.\n\nposterior.mean() / 2 \n```", "```py\n1.1666666666666665 \n```", "```py\nimport matplotlib.pyplot as plt\n\nqs = [1, 2, 3, 4]\nps = [22, 41, 24, 14]\nprior = Pmf(ps, qs)\nprior.bar(alpha=0.7)\n\nplt.xticks(qs, ['1 child', '2 children', '3 children', '4+ children'])\ndecorate(ylabel='PMF',\n         title='Distribution of family size') \n```", "```py\n# Solution\n\n# When you choose a person a random, you are more likely to get someone\n# from a bigger family; in fact, the chance of choosing someone from\n# any given family is proportional to the number of children\n\nlikelihood = qs\nposterior = prior * likelihood\nposterior.normalize()\nposterior \n```", "```py\n# Solution\n\n# The probability that they have a sibling is the probability\n# that they do not come from a family of 1 \n\n1 - posterior[1] \n```", "```py\n0.9051724137931034 \n```", "```py\n# Solution\n\n# Or we could use prob_gt again\n\nposterior.prob_gt(1) \n```", "```py\n0.9051724137931034 \n```", "```py\n# Solution\n\nhypos = [200, 2000]\nprior = Pmf(1, hypos) \n```", "```py\n# Solution\n\nlikelihood = 1/prior.qs\nposterior = prior * likelihood\nposterior.normalize()\nposterior \n```", "```py\n# Solution\n\n# According to this analysis, the probability is about 91% that our \n# civilization will be short-lived. \n# But this conclusion is based on a dubious prior.\n\n# And with so little data, the posterior depends strongly on the prior. \n# To see that, run this analysis again with a different prior, \n# and see what the results look like.\n\n# What do you think of the Doomsday argument? \n```"]