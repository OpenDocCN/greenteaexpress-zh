- en: Chapter 8  Estimation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://greenteapress.com/thinkstats2/html/thinkstats2009.html](https://greenteapress.com/thinkstats2/html/thinkstats2009.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The code for this chapter is in `estimation.py`. For information about downloading
    and working with this code, see Section [0.2](thinkstats2001.html#code).
  prefs: []
  type: TYPE_NORMAL
- en: 8.1  The estimation game
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s play a game. I think of a distribution, and you have to guess what it
    is. I’ll give you two hints: it’s a normal distribution, and here’s a random sample
    drawn from it:'
  prefs: []
  type: TYPE_NORMAL
- en: '`[-0.441, 1.774, -0.101, -1.138, 2.975, -2.138]`'
  prefs: []
  type: TYPE_NORMAL
- en: What do you think is the mean parameter, µ, of this distribution?
  prefs: []
  type: TYPE_NORMAL
- en: One choice is to use the sample mean, x, as an estimate of µ. In this example,
    x is 0.155, so it would be reasonable to guess µ = 0.155. This process is called
    estimation, and the statistic we used (the sample mean) is called an estimator.
  prefs: []
  type: TYPE_NORMAL
- en: Using the sample mean to estimate µ is so obvious that it is hard to imagine
    a reasonable alternative. But suppose we change the game by introducing outliers.
  prefs: []
  type: TYPE_NORMAL
- en: '*I’m thinking of a distribution.* It’s a normal distribution, and here’s a
    sample that was collected by an unreliable surveyor who occasionally puts the
    decimal point in the wrong place.'
  prefs: []
  type: TYPE_NORMAL
- en: '`[-0.441, 1.774, -0.101, -1.138, 2.975, -213.8]`'
  prefs: []
  type: TYPE_NORMAL
- en: Now what’s your estimate of µ? If you use the sample mean, your guess is -35.12\.
    Is that the best choice? What are the alternatives?
  prefs: []
  type: TYPE_NORMAL
- en: One option is to identify and discard outliers, then compute the sample mean
    of the rest. Another option is to use the median as an estimator.
  prefs: []
  type: TYPE_NORMAL
- en: Which estimator is best depends on the circumstances (for example, whether there
    are outliers) and on what the goal is. Are you trying to minimize errors, or maximize
    your chance of getting the right answer?
  prefs: []
  type: TYPE_NORMAL
- en: If there are no outliers, the sample mean minimizes the mean squared error (MSE).
    That is, if we play the game many times, and each time compute the error x − µ,
    the sample mean minimizes
  prefs: []
  type: TYPE_NORMAL
- en: '| MSE =  |'
  prefs: []
  type: TYPE_TB
- en: '&#124; 1 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; m &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|  ∑(x − µ)²  |'
  prefs: []
  type: TYPE_TB
- en: Where m is the number of times you play the estimation game, not to be confused
    with n, which is the size of the sample used to compute x.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a function that simulates the estimation game and computes the root
    mean squared error (RMSE), which is the square root of MSE:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Again, `n` is the size of the sample, and `m` is the number of times we play
    the game. `means` is the list of estimates based on x. `medians` is the list of
    medians.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the function that computes RMSE:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '`estimates` is a list of estimates; `actual` is the actual value being estimated.
    In practice, of course, we don’t know `actual`; if we did, we wouldn’t have to
    estimate it. The purpose of this experiment is to compare the performance of the
    two estimators.'
  prefs: []
  type: TYPE_NORMAL
- en: When I ran this code, the RMSE of the sample mean was 0.41, which means that
    if we use x to estimate the mean of this distribution, based on a sample with
    n=7, we should expect to be off by 0.41 on average. Using the median to estimate
    the mean yields RMSE 0.53, which confirms that x yields lower RMSE, at least for
    this example.
  prefs: []
  type: TYPE_NORMAL
- en: Minimizing MSE is a nice property, but it’s not always the best strategy. For
    example, suppose we are estimating the distribution of wind speeds at a building
    site. If the estimate is too high, we might overbuild the structure, increasing
    its cost. But if it’s too low, the building might collapse. Because cost as a
    function of error is not symmetric, minimizing MSE is not the best strategy.
  prefs: []
  type: TYPE_NORMAL
- en: As another example, suppose I roll three six-sided dice and ask you to predict
    the total. If you get it exactly right, you get a prize; otherwise you get nothing.
    In this case the value that minimizes MSE is 10.5, but that would be a bad guess,
    because the total of three dice is never 10.5\. For this game, you want an estimator
    that has the highest chance of being right, which is a maximum likelihood estimator
    (MLE). If you pick 10 or 11, your chance of winning is 1 in 8, and that’s the
    best you can do.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2  Guess the variance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*I’m thinking of a distribution.* It’s a normal distribution, and here’s a
    (familiar) sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '`[-0.441, 1.774, -0.101, -1.138, 2.975, -2.138]`'
  prefs: []
  type: TYPE_NORMAL
- en: What do you think is the variance, σ², of my distribution? Again, the obvious
    choice is to use the sample variance, S², as an estimator.
  prefs: []
  type: TYPE_NORMAL
- en: '| S² =  |'
  prefs: []
  type: TYPE_TB
- en: '&#124; 1 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; n &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|  ∑(x[i] − x)²  |'
  prefs: []
  type: TYPE_TB
- en: For large samples, S² is an adequate estimator, but for small samples it tends
    to be too low. Because of this unfortunate property, it is called a biased estimator.
    An estimator is unbiased if the expected total (or mean) error, after many iterations
    of the estimation game, is 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, there is another simple statistic that is an unbiased estimator
    of σ²:'
  prefs: []
  type: TYPE_NORMAL
- en: '| S[n−1]² =  |'
  prefs: []
  type: TYPE_TB
- en: '&#124; 1 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; n−1 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|  ∑(x[i] − x)²  |'
  prefs: []
  type: TYPE_TB
- en: For an explanation of why S² is biased, and a proof that S[n−1]² is unbiased,
    see [http://wikipedia.org/wiki/Bias_of_an_estimator](http://wikipedia.org/wiki/Bias_of_an_estimator).
  prefs: []
  type: TYPE_NORMAL
- en: The biggest problem with this estimator is that its name and symbol are used
    inconsistently. The name “sample variance” can refer to either S² or S[n−1]²,
    and the symbol S² is used for either or both.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a function that simulates the estimation game and tests the performance
    of S² and S[n−1]²:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Again, `n` is the sample size and `m` is the number of times we play the game.
    `np.var` computes S² by default and S[n−1]² if you provide the argument `ddof=1`,
    which stands for “delta degrees of freedom.” I won’t explain that term, but you
    can read about it at [http://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)](http://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)).
  prefs: []
  type: TYPE_NORMAL
- en: '`MeanError` computes the mean difference between the estimates and the actual
    value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: When I ran this code, the mean error for S² was -0.13\. As expected, this biased
    estimator tends to be too low. For S[n−1]², the mean error was 0.014, about 10
    times smaller. As `m` increases, we expect the mean error for S[n−1]² to approach
    0.
  prefs: []
  type: TYPE_NORMAL
- en: Properties like MSE and bias are long-term expectations based on many iterations
    of the estimation game. By running simulations like the ones in this chapter,
    we can compare estimators and check whether they have desired properties.
  prefs: []
  type: TYPE_NORMAL
- en: But when you apply an estimator to real data, you just get one estimate. It
    would not be meaningful to say that the estimate is unbiased; being unbiased is
    a property of the estimator, not the estimate.
  prefs: []
  type: TYPE_NORMAL
- en: After you choose an estimator with appropriate properties, and use it to generate
    an estimate, the next step is to characterize the uncertainty of the estimate,
    which is the topic of the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 8.3  Sampling distributions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Suppose you are a scientist studying gorillas in a wildlife preserve. You want
    to know the average weight of the adult female gorillas in the preserve. To weigh
    them, you have to tranquilize them, which is dangerous, expensive, and possibly
    harmful to the gorillas. But if it is important to obtain this information, it
    might be acceptable to weigh a sample of 9 gorillas. Let’s assume that the population
    of the preserve is well known, so we can choose a representative sample of adult
    females. We could use the sample mean, x, to estimate the unknown population mean,
    µ.
  prefs: []
  type: TYPE_NORMAL
- en: Having weighed 9 female gorillas, you might find x=90 kg and sample standard
    deviation, S=7.5 kg. The sample mean is an unbiased estimator of µ, and in the
    long run it minimizes MSE. So if you report a single estimate that summarizes
    the results, you would report 90 kg.
  prefs: []
  type: TYPE_NORMAL
- en: But how confident should you be in this estimate? If you only weigh n=9 gorillas
    out of a much larger population, you might be unlucky and choose the 9 heaviest
    gorillas (or the 9 lightest ones) just by chance. Variation in the estimate caused
    by random selection is called sampling error.
  prefs: []
  type: TYPE_NORMAL
- en: To quantify sampling error, we can simulate the sampling process with hypothetical
    values of µ and σ, and see how much x varies.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we don’t know the actual values of µ and σ in the population, we’ll use
    the estimates x and S. So the question we answer is: “If the actual values of
    µ and σ were 90 kg and 7.5 kg, and we ran the same experiment many times, how
    much would the estimated mean, x, vary?”'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following function answers that question:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '`mu` and `sigma` are the *hypothetical* values of the parameters. `n` is the
    sample size, the number of gorillas we measured. `m` is the number of times we
    run the simulation.'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/27c11a2c428f41ee8725ae5850149ff7.png)'
  prefs:
  - PREF_BQ
  type: TYPE_IMG
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '| Figure 8.1: Sampling distribution of x, with confidence interval. |'
  prefs:
  - PREF_BQ
  type: TYPE_TB
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '* * *'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In each iteration, we choose `n` values from a normal distribution with the
    given parameters, and compute the sample mean, `xbar`. We run 1000 simulations
    and then compute the distribution, `cdf`, of the estimates. The result is shown
    in Figure [8.1](#estimation1). This distribution is called the sampling distribution
    of the estimator. It shows how much the estimates would vary if we ran the experiment
    over and over.
  prefs: []
  type: TYPE_NORMAL
- en: The mean of the sampling distribution is pretty close to the hypothetical value
    of µ, which means that the experiment yields the right answer, on average. After
    1000 tries, the lowest result is 82 kg, and the highest is 98 kg. This range suggests
    that the estimate might be off by as much as 8 kg.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two common ways to summarize the sampling distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: Standard error (SE) is a measure of how far we expect the estimate to be off,
    on average. For each simulated experiment, we compute the error, x − µ, and then
    compute the root mean squared error (RMSE). In this example, it is roughly 2.5
    kg.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A confidence interval (CI) is a range that includes a given fraction of the
    sampling distribution. For example, the 90% confidence interval is the range from
    the 5th to the 95th percentile. In this example, the 90% CI is (86, 94) kg.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Standard errors and confidence intervals are the source of much confusion:'
  prefs: []
  type: TYPE_NORMAL
- en: People often confuse standard error and standard deviation. Remember that standard
    deviation describes variability in a measured quantity; in this example, the standard
    deviation of gorilla weight is 7.5 kg. Standard error describes variability in
    an estimate. In this example, the standard error of the mean, based on a sample
    of 9 measurements, is 2.5 kg.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One way to remember the difference is that, as sample size increases, standard
    error gets smaller; standard deviation does not.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: People often think that there is a 90% probability that the actual parameter,
    µ, falls in the 90% confidence interval. Sadly, that is not true. If you want
    to make a claim like that, you have to use Bayesian methods (see my book, Think
    Bayes).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The sampling distribution answers a different question: it gives you a sense
    of how reliable an estimate is by telling you how much it would vary if you ran
    the experiment again.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: It is important to remember that confidence intervals and standard errors only
    quantify sampling error; that is, error due to measuring only part of the population.
    The sampling distribution does not account for other sources of error, notably
    sampling bias and measurement error, which are the topics of the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 8.4  Sampling bias
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Suppose that instead of the weight of gorillas in a nature preserve, you want
    to know the average weight of women in the city where you live. It is unlikely
    that you would be allowed to choose a representative sample of women and weigh
    them.
  prefs: []
  type: TYPE_NORMAL
- en: A simple alternative would be “telephone sampling;” that is, you could choose
    random numbers from the phone book, call and ask to speak to an adult woman, and
    ask how much she weighs.
  prefs: []
  type: TYPE_NORMAL
- en: Telephone sampling has obvious limitations. For example, the sample is limited
    to people whose telephone numbers are listed, so it eliminates people without
    phones (who might be poorer than average) and people with unlisted numbers (who
    might be richer). Also, if you call home telephones during the day, you are less
    likely to sample people with jobs. And if you only sample the person who answers
    the phone, you are less likely to sample people who share a phone line.
  prefs: []
  type: TYPE_NORMAL
- en: If factors like income, employment, and household size are related to weight—and
    it is plausible that they are—the results of your survey would be affected one
    way or another. This problem is called sampling bias because it is a property
    of the sampling process.
  prefs: []
  type: TYPE_NORMAL
- en: This sampling process is also vulnerable to self-selection, which is a kind
    of sampling bias. Some people will refuse to answer the question, and if the tendency
    to refuse is related to weight, that would affect the results.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, if you ask people how much they weigh, rather than weighing them, the
    results might not be accurate. Even helpful respondents might round up or down
    if they are uncomfortable with their actual weight. And not all respondents are
    helpful. These inaccuracies are examples of measurement error.
  prefs: []
  type: TYPE_NORMAL
- en: When you report an estimated quantity, it is useful to report standard error,
    or a confidence interval, or both, in order to quantify sampling error. But it
    is also important to remember that sampling error is only one source of error,
    and often it is not the biggest.
  prefs: []
  type: TYPE_NORMAL
- en: 8.5  Exponential distributions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s play one more round of the estimation game. *I’m thinking of a distribution.*
    It’s an exponential distribution, and here’s a sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '`[5.384, 4.493, 19.198, 2.790, 6.122, 12.844]`'
  prefs: []
  type: TYPE_NORMAL
- en: What do you think is the parameter, λ, of this distribution?
  prefs: []
  type: TYPE_NORMAL
- en: In general, the mean of an exponential distribution is 1/λ, so working backwards,
    we might choose
  prefs: []
  type: TYPE_NORMAL
- en: '| L = 1 / x |'
  prefs: []
  type: TYPE_TB
- en: L is an estimator of λ. And not just any estimator; it is also the maximum likelihood
    estimator (see [http://wikipedia.org/wiki/Exponential_distribution#Maximum_likelihood](http://wikipedia.org/wiki/Exponential_distribution#Maximum_likelihood)).
    So if you want to maximize your chance of guessing λ exactly, L is the way to
    go.
  prefs: []
  type: TYPE_NORMAL
- en: But we know that x is not robust in the presence of outliers, so we expect L
    to have the same problem.
  prefs: []
  type: TYPE_NORMAL
- en: We can choose an alternative based on the sample median. The median of an exponential
    distribution is ln(2) / λ, so working backwards again, we can define an estimator
  prefs: []
  type: TYPE_NORMAL
- en: '| L[m] = ln(2) / m  |'
  prefs: []
  type: TYPE_TB
- en: where m is the sample median.
  prefs: []
  type: TYPE_NORMAL
- en: 'To test the performance of these estimators, we can simulate the sampling process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: When I run this experiment with λ=2, the RMSE of L is 1.1\. For the median-based
    estimator L[m], RMSE is 1.8\. We can’t tell from this experiment whether L minimizes
    MSE, but at least it seems better than L[m].
  prefs: []
  type: TYPE_NORMAL
- en: Sadly, it seems that both estimators are biased. For L the mean error is 0.33;
    for L[m] it is 0.45\. And neither converges to 0 as `m` increases.
  prefs: []
  type: TYPE_NORMAL
- en: It turns out that x is an unbiased estimator of the mean of the distribution,
    1 / λ, but L is not an unbiased estimator of λ.
  prefs: []
  type: TYPE_NORMAL
- en: 8.6  Exercises
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the following exercises, you can find starter code in `chap08ex.ipynb`.
    Solutions are in `chap08soln.py`
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 1
  prefs: []
  type: TYPE_NORMAL
- en: '*In this chapter we used* x *and median to estimate* µ*, and found that* x
    *yields lower MSE. Also, we used* S² *and* S[n−1]² *to estimate* σ*, and found
    that* S² *is biased and* S[n−1]² *unbiased.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Run similar experiments to see if* x *and median are biased estimates of*
    µ*. Also check whether* S² *or* S[n−1]² *yields a lower MSE.*'
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 2
  prefs: []
  type: TYPE_NORMAL
- en: '*Suppose you draw a sample with size* n=10 *from an exponential distribution
    with* λ=2*. Simulate this experiment 1000 times and plot the sampling distribution
    of the estimate* L*. Compute the standard error of the estimate and the 90% confidence
    interval.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Repeat the experiment with a few different values of* n *and make a plot of
    standard error versus* n*.*'
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 3
  prefs: []
  type: TYPE_NORMAL
- en: '*In games like hockey and soccer, the time between goals is roughly exponential.
    So you could estimate a team’s goal-scoring rate by observing the number of goals
    they score in a game. This estimation process is a little different from sampling
    the time between goals, so let’s see how it works.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Write a function that takes a goal-scoring rate, `lam`, in goals per game,
    and simulates a game by generating the time between goals until the total time
    exceeds 1 game, then returns the number of goals scored.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Write another function that simulates many games, stores the estimates of
    `lam`, then computes their mean error and RMSE.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Is this way of making an estimate biased? Plot the sampling distribution of
    the estimates and the 90% confidence interval. What is the standard error? What
    happens to sampling error for increasing values of `lam`?*'
  prefs: []
  type: TYPE_NORMAL
- en: 8.7  Glossary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'estimation: The process of inferring the parameters of a distribution from
    a sample.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'estimator: A statistic used to estimate a parameter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'mean squared error (MSE): A measure of estimation error.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'root mean squared error (RMSE): The square root of MSE, a more meaningful representation
    of typical error magnitude.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'maximum likelihood estimator (MLE): An estimator that computes the point estimate
    most likely to be correct.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'bias (of an estimator): The tendency of an estimator to be above or below the
    actual value of the parameter, when averaged over repeated experiments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'sampling error: Error in an estimate due to the limited size of the sample
    and variation due to chance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'sampling bias: Error in an estimate due to a sampling process that is not representative
    of the population.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'measurement error: Error in an estimate due to inaccuracy collecting or recording
    data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'sampling distribution: The distribution of a statistic if an experiment is
    repeated many times.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'standard error: The RMSE of an estimate, which quantifies variability due to
    sampling error (but not other sources of error).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'confidence interval: An interval that represents the expected range of an estimator
    if an experiment is repeated many times.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
