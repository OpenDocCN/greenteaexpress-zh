["```py\nimport pandas as pd\n\ndf = pd.read_csv('drp_scores.csv', skiprows=21, delimiter='\\t')\ndf.head(3) \n```", "```py\ngrouped = df.groupby('Treatment')\nresponses = {}\n\nfor name, group in grouped:\n    responses[name] = group['Response'] \n```", "```py\nfrom empiricaldist import Cdf\nfrom utils import decorate\n\nfor name, response in responses.items():\n    cdf = Cdf.from_seq(response)\n    cdf.plot(label=name)\n\ndecorate(xlabel='Score', \n         ylabel='CDF',\n         title='Distributions of test scores') \n```", "```py\nfrom empiricaldist import Pmf\n\ndef make_uniform(qs, name=None, **options):\n  \"\"\"Make a Pmf that represents a uniform distribution.\"\"\"\n    pmf = Pmf(1.0, qs, **options)\n    pmf.normalize()\n    if name:\n        pmf.index.name = name\n    return pmf \n```", "```py\nimport numpy as np\n\nqs = np.linspace(20, 80, num=101)\nprior_mu = make_uniform(qs, name='mean') \n```", "```py\nqs = np.linspace(5, 30, num=101)\nprior_sigma = make_uniform(qs, name='std') \n```", "```py\nfrom utils import make_joint\n\nprior = make_joint(prior_mu, prior_sigma) \n```", "```py\ndata = responses['Control']\ndata.shape \n```", "```py\n(23,) \n```", "```py\nmu_mesh, sigma_mesh, data_mesh = np.meshgrid(\n    prior.columns, prior.index, data)\n\nmu_mesh.shape \n```", "```py\n(101, 101, 23) \n```", "```py\nfrom scipy.stats import norm\n\ndensities = norm(mu_mesh, sigma_mesh).pdf(data_mesh)\ndensities.shape \n```", "```py\n(101, 101, 23) \n```", "```py\nlikelihood = densities.prod(axis=2)\nlikelihood.shape \n```", "```py\n(101, 101) \n```", "```py\nfrom utils import normalize\n\nposterior = prior * likelihood\nnormalize(posterior)\nposterior.shape \n```", "```py\n(101, 101) \n```", "```py\ndef update_norm(prior, data):\n  \"\"\"Update the prior based on data.\"\"\"\n    mu_mesh, sigma_mesh, data_mesh = np.meshgrid(\n        prior.columns, prior.index, data)\n\n    densities = norm(mu_mesh, sigma_mesh).pdf(data_mesh)\n    likelihood = densities.prod(axis=2)\n\n    posterior = prior * likelihood\n    normalize(posterior)\n\n    return posterior \n```", "```py\ndata = responses['Control']\nposterior_control = update_norm(prior, data) \n```", "```py\ndata = responses['Treated']\nposterior_treated = update_norm(prior, data) \n```", "```py\nimport matplotlib.pyplot as plt\nfrom utils import plot_contour\n\nplot_contour(posterior_control, cmap='Blues')\nplt.text(49.5, 18, 'Control', color='C0')\n\ncs = plot_contour(posterior_treated, cmap='Oranges')\nplt.text(57, 12, 'Treated', color='C1')\n\ndecorate(xlabel='Mean (mu)', \n         ylabel='Standard deviation (sigma)',\n         title='Joint posterior distributions of mu and sigma') \n```", "```py\nfrom utils import marginal\n\npmf_mean_control = marginal(posterior_control, 0)\npmf_mean_treated = marginal(posterior_treated, 0) \n```", "```py\npmf_mean_control.plot(label='Control')\npmf_mean_treated.plot(label='Treated')\n\ndecorate(xlabel='Population mean (mu)', \n         ylabel='PDF', \n         title='Posterior distributions of mu') \n```", "```py\nPmf.prob_gt(pmf_mean_treated, pmf_mean_control) \n```", "```py\n0.980479025187326 \n```", "```py\npmf_diff = Pmf.sub_dist(pmf_mean_treated, pmf_mean_control) \n```", "```py\nlen(pmf_mean_treated), len(pmf_mean_control), len(pmf_diff) \n```", "```py\n(101, 101, 879) \n```", "```py\npmf_diff.plot()\n\ndecorate(xlabel='Difference in population means', \n         ylabel='PDF', \n         title='Posterior distribution of difference in mu') \n```", "```py\ncdf_diff = pmf_diff.make_cdf() \n```", "```py\ncdf_diff.plot()\n\ndecorate(xlabel='Difference in population means', \n         ylabel='CDF', \n         title='Posterior distribution of difference in mu') \n```", "```py\nfrom scipy.stats import gaussian_kde\n\ndef kde_from_pmf(pmf, n=101):\n  \"\"\"Make a kernel density estimate for a PMF.\"\"\"\n    kde = gaussian_kde(pmf.qs, weights=pmf.ps)\n    qs = np.linspace(pmf.qs.min(), pmf.qs.max(), n)\n    ps = kde.evaluate(qs)\n    pmf = Pmf(ps, qs)\n    pmf.normalize()\n    return pmf \n```", "```py\nkde_diff = kde_from_pmf(pmf_diff) \n```", "```py\nkde_diff.plot()\n\ndecorate(xlabel='Difference in means', \n         ylabel='PDF', \n         title='Posterior distribution of difference in mu') \n```", "```py\npmf_diff.mean() \n```", "```py\n9.954413088940848 \n```", "```py\npmf_diff.credible_interval(0.9) \n```", "```py\narray([ 2.4, 17.4]) \n```", "```py\nmu = 42\nsigma = 17 \n```", "```py\nn = 20\nm = 41\ns = 18 \n```", "```py\ndist_m = norm(mu, sigma/np.sqrt(n)) \n```", "```py\nlike1 = dist_m.pdf(m)\nlike1 \n```", "```py\n0.10137915138497372 \n```", "```py\nt = n * s**2 / sigma**2\nt \n```", "```py\n22.422145328719722 \n```", "```py\nfrom scipy.stats import chi2\n\ndist_s = chi2(n-1) \n```", "```py\nlike2 = dist_s.pdf(t)\nlike2 \n```", "```py\n0.04736427909437004 \n```", "```py\nlike = like1 * like2\nlike \n```", "```py\n0.004801750420548287 \n```", "```py\nsummary = {}\n\nfor name, response in responses.items():\n    summary[name] = len(response), response.mean(), response.std()\n\nsummary \n```", "```py\n{'Control': (23, 41.52173913043478, 17.148733229699484),\n 'Treated': (21, 51.476190476190474, 11.00735684721381)} \n```", "```py\nn, m, s = summary['Control'] \n```", "```py\nmus, sigmas = np.meshgrid(prior.columns, prior.index)\nmus.shape \n```", "```py\n(101, 101) \n```", "```py\nlike1 = norm(mus, sigmas/np.sqrt(n)).pdf(m)\nlike1.shape \n```", "```py\n(101, 101) \n```", "```py\nts = n * s**2 / sigmas**2\nlike2 = chi2(n-1).pdf(ts)\nlike2.shape \n```", "```py\n(101, 101) \n```", "```py\nposterior_control2 = prior * like1 * like2\nnormalize(posterior_control2) \n```", "```py\ndef update_norm_summary(prior, data):\n  \"\"\"Update a normal distribution using summary statistics.\"\"\"\n    n, m, s = data\n    mu_mesh, sigma_mesh = np.meshgrid(prior.columns, prior.index)\n\n    like1 = norm(mu_mesh, sigma_mesh/np.sqrt(n)).pdf(m)\n    like2 = chi2(n-1).pdf(n * s**2 / sigma_mesh**2)\n\n    posterior = prior * like1 * like2\n    normalize(posterior)\n\n    return posterior \n```", "```py\ndata = summary['Treated']\nposterior_treated2 = update_norm_summary(prior, data) \n```", "```py\nplot_contour(posterior_control2, cmap='Blues')\nplt.text(49.5, 18, 'Control', color='C0')\n\ncs = plot_contour(posterior_treated2, cmap='Oranges')\nplt.text(57, 12, 'Treated', color='C1')\n\ndecorate(xlabel='Mean (mu)', \n         ylabel='Standard deviation (sigma)',\n         title='Joint posterior distributions of mu and sigma') \n```", "```py\nfrom utils import marginal\n\npmf_mean_control2 = marginal(posterior_control2, 0)\npmf_mean_treated2 = marginal(posterior_treated2, 0) \n```", "```py\npmf_mean_control.plot(color='C5', ls='--')\npmf_mean_control2.plot(label='Control')\npmf_mean_treated.plot(color='C5', ls='--')\npmf_mean_treated2.plot(label='Treated')\n\ndecorate(xlabel='Population mean', \n         ylabel='PDF', \n         title='Posterior distributions of mu') \n```", "```py\nmu = 42\nsigma = 17 \n```", "```py\ndist = norm(mu, sigma) \n```", "```py\nn = 20\nsamples = dist.rvs((1000, n))\nsamples.shape \n```", "```py\n(1000, 20) \n```", "```py\nsample_means = samples.mean(axis=1)\nsample_means.shape \n```", "```py\n(1000,) \n```", "```py\ndef pmf_from_dist(dist, low, high):\n  \"\"\"Make a discrete approximation of a continuous distribution.\n\n dist: SciPy dist object\n low: low end of range\n high: high end of range\n\n returns: normalized Pmf\n \"\"\"\n    qs = np.linspace(low, high, 101)\n    ps = dist.pdf(qs)\n    pmf = Pmf(ps, qs)\n    pmf.normalize()\n    return pmf \n```", "```py\nlow = dist_m.mean() - dist_m.std() * 3\nhigh = dist_m.mean() + dist_m.std() * 3\n\npmf_m = pmf_from_dist(dist_m, low, high) \n```", "```py\nfrom utils import kde_from_sample\n\nqs = pmf_m.qs\npmf_sample_means = kde_from_sample(sample_means, qs) \n```", "```py\npmf_m.plot(label='Theoretical distribution',\n           ls=':', color='C5')\npmf_sample_means.plot(label='KDE of sample means')\n\ndecorate(xlabel='Mean score',\n         ylabel='PDF',\n         title='Distribution of the mean') \n```", "```py\nsample_stds = samples.std(axis=1)\nsample_stds.shape \n```", "```py\n(1000,) \n```", "```py\ntransformed = n * sample_stds**2 / sigma**2 \n```", "```py\nfrom scipy.stats import chi2\n\ndist_s = chi2(n-1) \n```", "```py\nlow = 0\nhigh = dist_s.mean() + dist_s.std() * 4\n\npmf_s = pmf_from_dist(dist_s, low, high) \n```", "```py\nqs = pmf_s.qs\npmf_sample_stds = kde_from_sample(transformed, qs) \n```", "```py\npmf_s.plot(label='Theoretical distribution',\n           ls=':', color='C5')\npmf_sample_stds.plot(label='KDE of sample std',\n                     color='C1')\n\ndecorate(xlabel='Standard deviation of scores',\n         ylabel='PDF',\n         title='Distribution of standard deviation') \n```", "```py\nnp.corrcoef(sample_means, sample_stds)[0][1] \n```", "```py\n-0.027451907688034228 \n```", "```py\nimport seaborn as sns\n\nsns.kdeplot(x=sample_means, y=sample_stds)\n\ndecorate(xlabel='Mean (mu)',\n         ylabel='Standard deviation (sigma)',\n         title='Joint distribution of mu and sigma') \n```", "```py\n# Solution\n\npmf_std_control = marginal(posterior_control, 1)\npmf_std_treated = marginal(posterior_treated, 1) \n```", "```py\n# Solution\n\npmf_std_control.plot(label='Control')\npmf_std_treated.plot(label='Treated')\n\ndecorate(xlabel='Population standard deviation', \n         ylabel='PDF', \n         title='Posterior distributions of sigma') \n```", "```py\n# Solution\n\nPmf.prob_gt(pmf_std_control, pmf_std_treated) \n```", "```py\n0.9685103375300469 \n```", "```py\n# Solution\n\npmf_diff2 = Pmf.sub_dist(pmf_std_control, pmf_std_treated) \n```", "```py\n# Solution\n\npmf_diff2.mean() \n```", "```py\n6.41717132817218 \n```", "```py\n# Solution\n\npmf_diff2.credible_interval(0.9) \n```", "```py\narray([ 1\\. , 12.5]) \n```", "```py\n# Solution\n\nkde_from_pmf(pmf_diff2).plot()\n\ndecorate(xlabel='Difference in population standard deviation', \n         ylabel='PDF', \n         title='Posterior distributions of difference in sigma') \n```", "```py\ndef sample_joint(joint, size):\n  \"\"\"Draw a sample from a joint distribution.\n\n joint: DataFrame representing a joint distribution\n size: sample size\n \"\"\"\n    pmf = Pmf(joint.transpose().stack())\n    return pmf.choice(size) \n```", "```py\nsample_treated = sample_joint(posterior_treated, 1000)\nsample_treated.shape \n```", "```py\n(1000,) \n```", "```py\nsample_control = sample_joint(posterior_control, 1000)\nsample_control.shape \n```", "```py\n(1000,) \n```", "```py\n# Solution\n\ndef cohen_effect(pair1, pair2):\n  \"\"\"Compute Cohen's effect size for difference in means.\n\n pair1: tuple of (mu1, sigma1)\n pair2: tuple of (mu2, sigma2)\n\n return: float\n \"\"\"\n    mu1, sigma1 = pair1 \n    mu2, sigma2 = pair2\n    sigma = (sigma1 + sigma2) / 2\n    return (mu1 - mu2) / sigma \n```", "```py\n# Solution\n\ncohen_effect(sample_treated[0], sample_control[0]) \n```", "```py\n0.7603960396039605 \n```", "```py\n# Solution\n\nds = []\nfor pair1, pair2 in zip(sample_treated, sample_control):\n    d = cohen_effect(pair1, pair2)\n    ds.append(d) \n```", "```py\n# Solution\n\ncdf = Cdf.from_seq(ds)\ncdf.plot()\n\ndecorate(xlabel='Cohen effect size',\n         ylabel='CDF',\n         title='Posterior distributions of effect size') \n```", "```py\n# Solution\n\ncdf.mean() \n```", "```py\n0.6623391688256146 \n```", "```py\n# Solution\n\ncdf.credible_interval(0.9) \n```", "```py\narray([0.08648649, 1.17647059]) \n```", "```py\n# Solution\n\n# Based on trial and error, here's a range of\n# values for the prior\n\nhypos = np.linspace(1, 51, 101) \n```", "```py\n# Solution\n\n# Here are the probabilities of a score greater than 90\n# for each hypothetical value of sigma.\n\nfrom scipy.stats import norm\n\npgt90 = norm(81, hypos).sf(90)\npgt90.shape \n```", "```py\n(101,) \n```", "```py\n# Solution\n\n# And here's the chance that 5 out of 25 people\n# get a score greater than 90\n\nfrom scipy.stats import binom\n\nlikelihood1 = binom(25, pgt90).pmf(5)\nlikelihood1.shape \n```", "```py\n(101,) \n```", "```py\n# Solution\n\n# Here's the first update\n\nprior = Pmf(1, hypos)\nposterior = prior * likelihood1\nposterior.normalize() \n```", "```py\n5.299480018256258 \n```", "```py\n# Solution\n\n# Here's the first posterior.\n\nposterior.plot()\ndecorate(xlabel='Standard deviation (sigma)',\n         ylabel='PMF',\n         title='Posterior distribution of sigma') \n```", "```py\n# Solution\n\n# Here's the probability of a score greater than 60\n\npgt60s = norm(81, hypos).sf(60) \n```", "```py\n# Solution\n\n# And here's the probability that all 25 students exceed 60\n\nlikelihood2 = pgt60s ** 25 \n```", "```py\n# Solution\n\nplt.plot(hypos, likelihood2)\ndecorate(xlabel='Standard deviation (sigma)',\n         ylabel='Likelihood',\n         title='Likelihood function') \n```", "```py\n# Solution\n\n# Here's the posterior after both updates\n\nprior = Pmf(1, hypos)\nprior.normalize()\nposterior2 = prior * likelihood1 * likelihood2\nposterior2.normalize() \n```", "```py\n0.01425455531129565 \n```", "```py\n# Solution\n\nposterior.plot(label='Posterior 1')\nposterior2.plot(label='Posterior 2')\n\ndecorate(xlabel='Standard deviation (sigma)',\n         ylabel='PMF',\n         title='Posterior distribution of sigma') \n```", "```py\n# Solution\n\nposterior.mean(), posterior2.mean() \n```", "```py\n(18.150261186811548, 10.189707962198526) \n```", "```py\n# Solution\n\nposterior2.credible_interval(0.9) \n```", "```py\narray([ 7., 15.]) \n```", "```py\ndef get_posterior_cv(joint):\n  \"\"\"Get the posterior distribution of CV.\n\n joint: joint distribution of mu and sigma\n\n returns: Pmf representing the smoothed posterior distribution\n \"\"\"\n    pmf_mu = marginal(joint, 0)\n    pmf_sigma = marginal(joint, 1)\n    pmf_cv = Pmf.div_dist(pmf_sigma, pmf_mu)\n    return kde_from_pmf(pmf_cv) \n```", "```py\n# Solution\n\nn = 154407\nmean = 178\nstd = 8.27 \n```", "```py\n# Solution\n\nqs = np.linspace(mean-0.1, mean+0.1, num=101)\nprior_mu = make_uniform(qs, name='mean')\n\nqs = np.linspace(std-0.1, std+0.1, num=101)\nprior_sigma = make_uniform(qs, name='std')\n\nprior = make_joint(prior_mu, prior_sigma) \n```", "```py\n# Solution\n\ndata = n, mean, std\nposterior_male = update_norm_summary(prior, data)\nplot_contour(posterior_male, cmap='Blues')\n\ndecorate(xlabel='Mean (mu)', \n         ylabel='Standard deviation (sigma)',\n         title='Joint distribution of mu and sigma') \n```", "```py\n# Solution\n\nn = 254722\nmean = 163\nstd = 7.75 \n```", "```py\n# Solution\n\nqs = np.linspace(mean-0.1, mean+0.1, num=101)\nprior_mu = make_uniform(qs, name='mean')\n\nqs = np.linspace(std-0.1, std+0.1, num=101)\nprior_sigma = make_uniform(qs, name='std')\n\nprior = make_joint(prior_mu, prior_sigma) \n```", "```py\n# Solution\n\ndata = n, mean, std\nposterior_female = update_norm_summary(prior, data)\nplot_contour(posterior_female, cmap='Oranges'); \n```", "```py\n# Solution\n\npmf_cv_male = get_posterior_cv(posterior_male)\nkde_from_pmf(pmf_cv_male).plot()\n\npmf_cv_female = get_posterior_cv(posterior_female)\nkde_from_pmf(pmf_cv_female).plot()\n\ndecorate(xlabel='Coefficient of variation',\n         ylabel='PDF',\n         title='Posterior distributions of CV') \n```", "```py\n# Solution\n\nratio_cv = Pmf.div_dist(pmf_cv_female, pmf_cv_male)\nratio_cv.max_prob() \n```", "```py\n1.0233615721208176 \n```", "```py\n# Solution\n\nratio_cv.credible_interval(0.9) \n```", "```py\narray([1.0193799 , 1.02734473]) \n```"]