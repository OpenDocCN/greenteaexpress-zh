- en: Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://allendowney.github.io/ElementsOfDataScience/10_regression.html](https://allendowney.github.io/ElementsOfDataScience/10_regression.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Click here to run this notebook on Colab](https://colab.research.google.com/github/AllenDowney/ElementsOfDataScience/blob/master/10_regression.ipynb)
    or [click here to download it](https://github.com/AllenDowney/ElementsOfDataScience/raw/master/10_regression.ipynb).'
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter we used simple linear regression to quantify the relationship
    between two variables. In this chapter we’ll get farther into regression, including
    multiple regression and one of my all-time favorite tools, logistic regression.
  prefs: []
  type: TYPE_NORMAL
- en: These tools will allow us to explore relationships among sets of variables.
    As an example, we will use data from the General Social Survey (GSS) to explore
    the relationship between income, education, age, and sex. But first let’s understand
    the limits of simple regression.
  prefs: []
  type: TYPE_NORMAL
- en: Limits of Simple Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In a previous exercise, you made a scatter plot of vegetable consumption as
    a function of income, and plotted a line of best fit. Here’s what it looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/87d9d2b0287e9053ae7755d9809f8756.png)'
  prefs: []
  type: TYPE_IMG
- en: The slope of the line is 0.07, which means that the difference between the lowest
    and highest income brackets is about 0.49 servings per day. So that’s not a very
    big difference.
  prefs: []
  type: TYPE_NORMAL
- en: But it was an arbitrary choice to plot vegetables as a function of income. We
    could have plotted it the other way around, like this.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6c47c5ecca914eda960557904adc81f5.png)'
  prefs: []
  type: TYPE_IMG
- en: The slope of this line is about 0.2, which means that the difference between
    0 and 10 servings per day is about 2 income levels, roughly from level 5 to level
    7.
  prefs: []
  type: TYPE_NORMAL
- en: And the difference between income levels 5 and 7 is about $30,000 per year,
    which is substantial.
  prefs: []
  type: TYPE_NORMAL
- en: So if we use vegetable consumption to predict income, we see a big difference.
    But when we used income to predict vegetable consumption, we saw a small difference.
  prefs: []
  type: TYPE_NORMAL
- en: This example shows that regression is not symmetric; the regression of A onto
    B is not the same as the regression of B onto A.
  prefs: []
  type: TYPE_NORMAL
- en: We can see that more clearly by putting the two figures side by side and plotting
    both regression lines on both figures.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cee1b89025f8abd15455316ed120bb5a.png)'
  prefs: []
  type: TYPE_IMG
- en: They are different because they are based on different assumptions.
  prefs: []
  type: TYPE_NORMAL
- en: On the left, we treat income as a known quantity and vegetable consumption as
    random.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the right, we treat vegetable consumption as known and income as random.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When you run a regression model, you make decisions about how to treat the data,
    and those decisions affect the results you get.
  prefs: []
  type: TYPE_NORMAL
- en: This example demonstrates another point, which is that regression doesn’t tell
    you much about causation.
  prefs: []
  type: TYPE_NORMAL
- en: If you think people with lower income can’t afford vegetables, you might look
    at the figure on the left and conclude that it doesn’t make much difference.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you think better diet increases income, the figure on the right might make
    you think it does.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: But in general, regression can’t tell you what causes what. If you see a relationship
    between any two variables, A and B, the reason for the relationship might be that
    A causes B, or B causes A, or there might be other factors that cause both A and
    B. Regression alone can’t tell you which way it goes.
  prefs: []
  type: TYPE_NORMAL
- en: However, we have tools for quantifying relationships among multiple variables;
    one of the most important is multiple regression.
  prefs: []
  type: TYPE_NORMAL
- en: Regression with StatsModels
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SciPy doesn’t do multiple regression, so we’ll to switch to a new library, StatsModels.
    Here’s the import statement.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: For the first example, we’ll load data from the Behavioral Risk Factor Surveillance
    Survey (BRFSS), which we saw in the previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Now we can use StatsModels to fit a regression model. We’ll start with one of
    the examples from the previous chapter, the relationship between income and vegetable
    consumption. We’ll check that the results from StatsModels are the same as the
    results from SciPy. Then we’ll move on to multiple regression.
  prefs: []
  type: TYPE_NORMAL
- en: The function we’ll use is `ols`, which stands for “ordinary least squares”,
    another name for regression.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The first argument is a **formula string** that specifies that we want to regress
    income as a function of vegetable consumption. The second argument is the BRFSS
    `DataFrame`. The names in the formula string correspond to columns in the `DataFrame`.
  prefs: []
  type: TYPE_NORMAL
- en: The result from `ols` is an object that represents the model; it provides a
    function called `fit` that does the actual computation.
  prefs: []
  type: TYPE_NORMAL
- en: The result is a `RegressionResultsWrapper`, which contains several attributes;
    the first one we’ll look at is `params`, which contains the estimated intercept
    and the slope associated with `_VEGESU1`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The results from Statsmodels are the same as the results we got from SciPy,
    so that’s good!
  prefs: []
  type: TYPE_NORMAL
- en: There are only two variables in this example, so it is still simple regression.
    In the next section we’ll move on to multiple regression. But first, some exercises.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise:** In the BRFSS dataset, there is a strong relationship between
    vegetable consumption and income. The income of people who eat 8 servings of vegetables
    per day is double the income of people who eat none, on average.'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following conclusions can we draw from this data?
  prefs: []
  type: TYPE_NORMAL
- en: A. Eating a good diet leads to better health and higher income.
  prefs: []
  type: TYPE_NORMAL
- en: B. People with higher income can afford a better diet.
  prefs: []
  type: TYPE_NORMAL
- en: C. People with high income are more likely to be vegetarians.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise:** Let’s run a regression using SciPy and StatsModels, and confirm
    we get the same results.'
  prefs: []
  type: TYPE_NORMAL
- en: Compute the regression of `_VEGESU1` as a function of `INCOME2` using SciPy’s
    `linregress()`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the regression of `_VEGESU1` as a function of `INCOME2` using StatsModels’
    `smf.ols()`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Note: `linregress` does not handle `NaN` values, so you will have to use `dropna`
    to select the rows with valid data.'
  prefs: []
  type: TYPE_NORMAL
- en: Multiple Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have StatsModels, getting from simple to multiple regression is
    easy. As an example, we’ll use data from the General Social Survey (GSS) and we’ll
    explore variables that are related to income.
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s load the GSS data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are the first few rows of `gss`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '|  | YEAR | ID_ | AGE | EDUC | SEX | GUNLAW | GRASS | REALINC |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 1972 | 1 | 23.0 | 16.0 | 2 | 1.0 | NaN | 18951.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1972 | 2 | 70.0 | 10.0 | 1 | 1.0 | NaN | 24366.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 1972 | 3 | 48.0 | 12.0 | 2 | 1.0 | NaN | 24366.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 1972 | 4 | 27.0 | 17.0 | 2 | 1.0 | NaN | 30458.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 1972 | 5 | 61.0 | 12.0 | 2 | 1.0 | NaN | 50763.0 |'
  prefs: []
  type: TYPE_TB
- en: We’ll start with another simple regression, estimating the parameters of real
    income as a function of years of education.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: On the left side of the formula string, `REALINC` is the variable we are trying
    to predict; on the right, `EDUC` is the variable we are using to inform the predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The estimated slope is about `3450`, which means that each additional year
    of education is associated with an additional $3450 of income. But income also
    depends on age, so it would be good to include that in the model, too. Here’s
    how:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: On the right side of the formula string, you can list as many variables as you
    like, in this case, education and age. The `plus` sign indicates that we expect
    the contributions of the two variables to be additive, which is a common assumption
    for models like this.
  prefs: []
  type: TYPE_NORMAL
- en: The estimated slope for `EDUC` is a little less than what we saw before, about
    $3514 per year.
  prefs: []
  type: TYPE_NORMAL
- en: The estimated slope for `AGE` is only about $54 per year, which is surprisingly
    small.
  prefs: []
  type: TYPE_NORMAL
- en: Grouping by Age
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To see what’s going on, let’s look more closely at the relationship between
    income and age. We’ll use a Pandas method we have not seen before, called `groupby`,
    to divide the `DataFrame` into age groups.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The result is a `GroupBy` object that contains one group for each value of `age`.
    The `GroupBy` object behaves like a `DataFrame` in many ways. You can use brackets
    to select a column, like `REALINC` in this example, and then invoke a method like
    `mean`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The result is a Pandas `Series` that contains the mean income for each age group,
    which we can plot like this.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/10_regression_38_0.png](../Images/3d9e6bb1d0505e2b65de9033c01a9819.png)'
  prefs: []
  type: TYPE_IMG
- en: Average income increases from age 20 to age 50, then starts to fall.
  prefs: []
  type: TYPE_NORMAL
- en: And that explains why the estimated slope is so small, because the relationship
    is non-linear. Remember that correlation and simple regression can’t measure non-linear
    relationships.
  prefs: []
  type: TYPE_NORMAL
- en: But multiple regression can! To describe a non-linear relationship, one option
    is to add a new variable that is a non-linear combination of other variables.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, we’ll create a new variable called `AGE2` that equals `AGE` squared.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Now we can run a regression with both `age` and `age2` on the right side.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: In this model, the slope associated with `AGE` is substantial, about $1760 per
    year.
  prefs: []
  type: TYPE_NORMAL
- en: The slope associated with `AGE2` is about -$17, but that’s harder to interpret.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we’ll see methods to interpret multivariate models and
    visualize the results. But first, here are two exercises where you can practice
    using `groupby` and `ols`.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise:** To get a closer look at the relationship between income and education,
    let’s use the variable `EDUC` to group the data, then plot mean income in each
    group.'
  prefs: []
  type: TYPE_NORMAL
- en: Group `gss` by `EDUC`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From the resulting `GroupBy` object, extract `REALINC` and compute the mean.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Plot mean income in each education group as a scatter plot.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What can you say about the relationship between education and income? Does it
    look like a linear relationship?
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise:** The graph in the previous exercise suggests that the relationship
    between income and education is non-linear. So let’s try fitting a non-linear
    model.'
  prefs: []
  type: TYPE_NORMAL
- en: Add a column named `EDUC2` to the `gss` DataFrame; it should contain the values
    from `EDUC` squared.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run a regression model that uses `EDUC`, `EDUC2`, `age`, and `age2` to predict
    `REALINC`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing regression results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section we ran a multiple regression model to characterize the
    relationships between income, age, and education. Because the model includes quadratic
    terms, the parameters are hard to interpret. For example, you might notice that
    the parameter for `EDUC` is negative, and that might be a surprise, because it
    suggests that higher education is associated with lower income.
  prefs: []
  type: TYPE_NORMAL
- en: But the parameter for `EDUC2` is positive, and that makes a big difference.
    In this section we’ll see a way to interpret the model visually and validate it
    against data.
  prefs: []
  type: TYPE_NORMAL
- en: Here’s the model from the previous exercise.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Sometimes we can understand a model by looking at its parameters, but often
    it is better to look at its predictions.
  prefs: []
  type: TYPE_NORMAL
- en: The regression results provide a method called `predict` that uses the model
    to generate predictions. It takes a `DataFrame` as a parameter and returns a `Series`
    with a prediction for each row in the `DataFrame`. To use it, we’ll create a new
    `DataFrame` with `AGE` running from 18 to 89, and `AGE2` set to `AGE` squared.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Next, we’ll pick a level for `EDUC`, like 12 years, which is the most common
    value. When you assign a single value to a column in a `DataFrame`, Pandas makes
    a copy for each row.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Then we can use `results` to predict the average income for each age group,
    holding education constant.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The result from `predict` is a `Series` with one prediction for each row. So
    we can plot it with age on the \(x\)-axis and the predicted income for each age
    group on the \(y\)-axis. And we can plot the data for comparison.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/10_regression_57_0.png](../Images/b9b16ddf823f1a93bc564c106d4dd17e.png)'
  prefs: []
  type: TYPE_IMG
- en: The dots show the average income in each age group. The line shows the predictions
    generated by the model, holding education constant. This plot shows the shape
    of the model, a downward-facing parabola.
  prefs: []
  type: TYPE_NORMAL
- en: We can do the same thing with other levels of education, like 14 years, which
    is the nominal time to earn an Associate’s degree, and 16 years, which is the
    nominal time to earn a Bachelor’s degree.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/10_regression_60_0.png](../Images/89166cad7bdbadc5743465639c3577ac.png)'
  prefs: []
  type: TYPE_IMG
- en: The lines show mean income, as predicted by the model, as a function of age,
    for three levels of education. This visualization helps validate the model, since
    we can compare the predictions with the data. And it helps us interpret the model
    since we can see the separate contributions of age and education.
  prefs: []
  type: TYPE_NORMAL
- en: In the exercises, you’ll have a chance to run a multiple regression, generate
    predictions, and visualize the results.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise:** At this point, we have a model that predicts income using age,
    education, and sex.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see what it predicts for different levels of education, holding `AGE`
    constant.
  prefs: []
  type: TYPE_NORMAL
- en: Create an empty `DataFrame` named `df`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using `np.linspace()`, add a variable named `EDUC` to `df` with a range of values
    from `0` to `20`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add a variable named `AGE` with the constant value `30`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use `df` to generate predicted income as a function of education.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Exercise:** Now let’s visualize the results from the previous exercise!'
  prefs: []
  type: TYPE_NORMAL
- en: Group the GSS data by `EDUC` and compute the mean income in each education group.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Plot mean income for each education group as a scatter plot.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Plot the predictions from the previous exercise.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do the predictions compare with the data?
  prefs: []
  type: TYPE_NORMAL
- en: '**Optional Exercise:** Extend the previous exercise to include predictions
    for a few other age levels.'
  prefs: []
  type: TYPE_NORMAL
- en: Categorical Variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most of the variables we have used so far — like income, age, and education
    — are numerical. But variables like sex and race are **categorical**; that is,
    each respondent belongs to one of a specified set of categories. If there are
    only two categories, we would say the variable is **binary**.
  prefs: []
  type: TYPE_NORMAL
- en: 'With StatsModels, it is easy to include a categorical variable as part of a
    regression model. Here’s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: In the formula string, the letter `C` indicates that `SEX` is a categorical
    variable.
  prefs: []
  type: TYPE_NORMAL
- en: The regression treats the value `SEX=1`, which is male, as the default, and
    reports the difference associated with the value `SEX=2`, which is female. So
    this result indicates that income for women is about $4156 less than for men,
    after controlling for age and education.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, we added a categorical variables on the right side
    of a regression formula; that is, we used it as a predictive variables.
  prefs: []
  type: TYPE_NORMAL
- en: But what if the categorical variable is on the left side of the regression formula;
    that is, it’s the value we are trying to predict? In that case, we can use **logistic
    regression**.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, one of the questions in the General Social Survey asks “Would
    you favor or oppose a law which would require a person to obtain a police permit
    before he or she could buy a gun?” The responses are in a column called `GUNLAW`;
    here are the values.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '`1` means yes and `2` means no, so most respondents are in favor.'
  prefs: []
  type: TYPE_NORMAL
- en: To explore the relationship between this variable and factors like age, sex,
    and education, we can use StatsModels, which provides a function that does logistic
    regression.
  prefs: []
  type: TYPE_NORMAL
- en: To use it, we have to recode the variable so `1` means “yes” and `0` means “no”.
    We can do that by replacing `2` with `0`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Now we can run the regression. Instead of `ols()`, we use `logit()`, which is
    named for the logit function, which is related to logistic regression.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Estimating the parameters for the logistic model is an iterative process, so
    the output contains information about the number of iterations. Other than that,
    everything is the same as what we have seen before. And here are the results.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The parameters are in the form of **log odds**, which you may or may not be
    familiar with. I won’t explain them in detail here, except to say that positive
    values are associated with things that make the outcome more likely, and negative
    values make the outcome less likely.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the parameter associated with `SEX=2` is 0.75, which indicates
    that women are more likely to support this form of gun control. To see how much
    more likely, we can generate predictions, as we did with linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, we’ll generate predictions for different ages and sexes, with
    education held constant. First we need a `DataFrame` with `AGE` and `EDUC`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Then we can compute `AGE2` and `EDUC2`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: We can generate predictions for men like this.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: And for women like this.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Now, to visualize the results, we’ll start by plotting the data. As we’ve done
    before, we’ll divide the respondents into age groups and compute the mean in each
    group. The mean of a binary variable is the fraction of people in favor.
  prefs: []
  type: TYPE_NORMAL
- en: Then we can plot the predictions, for men and women, as a function of age.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '![_images/10_regression_89_0.png](../Images/7a178679eac22806f63ea7ef315d4f31.png)'
  prefs: []
  type: TYPE_IMG
- en: According to the model, people near age 50 are least likely to support gun control
    (at least as this question was posed). And women are more likely to support it
    than men, by almost 15 percentage points.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression is a powerful tool for exploring relationships between a
    binary variable and the factors that predict it. In the exercises, you’ll explore
    the factors that predict support for legalizing marijuana.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise:** Let’s use logistic regression to predict a binary variable. Specifically,
    we’ll use age, sex, and education level to predict support for legalizing marijuana
    in the U.S.'
  prefs: []
  type: TYPE_NORMAL
- en: In the GSS dataset, the variable `GRASS` records the answer to the question
    “Do you think the use of marijuana should be made legal or not?”
  prefs: []
  type: TYPE_NORMAL
- en: First, use `replace` to recode the `GRASS` column so that `1` means yes and
    `0` means no. Use `value_counts` to check.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, use `smf.logit()` to predict `GRASS` using the variables `AGE`, `AGE2`,
    `EDUC`, and `EDUC2`, along with `SEX` as a categorical variable. Display the parameters.
    Are men or women more likely to support legalization?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To generate predictions, start with an empty DataFrame. Add a column called
    `AGE` that contains a sequence of values from 18 to 89\. Add a column called `EDUC`
    and set it to 12 years. Then compute a column, `AGE2`, which is the square of
    `AGE`, and a column, `EDUC2`, which is the square of `EDUC`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use `predict` to generate predictions for men (`SEX=1`) and women (`SEX=2`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate a plot that shows (1) the average level of support for legalizing marijuana
    in each age group, (2) the level of support the model predicts for men as a function
    of age, and (3) the level of support predicted for women as a function of age.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At this point, I’d like to summarize the topics we’ve covered so far, and make
    some connections that might clarify the big picture.
  prefs: []
  type: TYPE_NORMAL
- en: 'A central theme of this book is **exploratory data analysis**, which a process
    and set of tools for exploring a dataset, visualizing distributions, and discovering
    relationships between variables. The last four chapters demonstrate the steps
    of this process:'
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 7 is about importing and cleaning data, and checking for errors and
    other special conditions. This might not be the most exciting part of the process,
    but time spent validating data can save you from embarrassing errors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chapter 8 is about exploring variables one at a time, visualizing distributions
    using PMFs, CDFs, and KDE, and choosing appropriate summary statistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In Chapter 9 we explored relationships between variables two at a time, using
    scatter plots and other visualizations; and we quantified those relationships
    using correlation and simple regression.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, in this chapter, we explored multivariate relationships using multiple
    regression and logistic regression.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In Chapter 7, we looked at the distribution of birth weights from the National
    Survey of Family Growth. If you only remember one thing, remember the 99 pound
    babies, and how much it can affect your results if you don’t validate the data.
  prefs: []
  type: TYPE_NORMAL
- en: In Chapter 8 we looked at the distributions of age, income, and other variables
    from the General Social Survey. I recommended using CDFs as the best way to explore
    distributions. But when you present to audiences that are not familiar with CDFs,
    you can use PMFs if there are a small number of unique values, and KDE if there
    are a lot.
  prefs: []
  type: TYPE_NORMAL
- en: In Chapter 9 we looked at heights and weights from the BRFSS, and developed
    several ways to visualize relationships between variables, including scatter plots,
    violin plots, and box plots.
  prefs: []
  type: TYPE_NORMAL
- en: We used the coefficient of correlation to quantify the strength of a relationship.
    We also used simple regression to estimate slope, which is often what we care
    more about, not correlation.
  prefs: []
  type: TYPE_NORMAL
- en: But remember that both of these methods only capture linear relationships; if
    the relationship is non-linear, they can be misleading. Always look at a visualization,
    like a scatter plot, before computing correlation or simple regression.
  prefs: []
  type: TYPE_NORMAL
- en: In Chapter 10 we used multiple regression to add control variables and to describe
    non-linear relationships. And finally we used logistic regression to explain and
    predict binary variables.
  prefs: []
  type: TYPE_NORMAL
- en: We moved through a lot of material quickly, but if you practice and apply these
    methods to other questions and other datasets, you will learn more as you go.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will move on to a new topic, resampling, which is a
    versatile tool for statistical inference.
  prefs: []
  type: TYPE_NORMAL
