["```py\nimport matplotlib.pyplot as plt\n\ndef legend(**options):\n  \"\"\"Make a legend only if there are labels.\"\"\"\n    handles, labels = plt.gca().get_legend_handles_labels()\n    if len(labels):\n        plt.legend(**options) \n```", "```py\ndef decorate(**options):\n    plt.gca().set(**options)\n    legend()\n    plt.tight_layout() \n```", "```py\nfrom empiricaldist import Cdf\n\ndef compare_cdf(pmf, sample):\n    pmf.make_cdf().step(label='grid')\n    Cdf.from_seq(sample).plot(label='mcmc')\n    print(pmf.mean(), sample.mean())\n    decorate() \n```", "```py\nfrom empiricaldist import Pmf\n\ndef make_pmf(ps, qs, name):\n    pmf = Pmf(ps, qs)\n    pmf.normalize()\n    pmf.index.name = name\n    return pmf \n```", "```py\nimport os\n\nfilename = 'DeathHeartAttackManhattan.csv'\nif not os.path.exists(filename):\n    !wget  https://github.com/AllenDowney/BayesianInferencePyMC/raw/main/DeathHeartAttackManhattan.csv \n```", "```py\nimport pandas as pd\n\ndf = pd.read_csv(filename)\ndf \n```", "```py\ndata_ns = df['Cases'].values\ndata_ks = df['Deaths'].values \n```", "```py\nimport pymc3 as pm\n\ndef make_model():\n    with pm.Model() as model:\n        mu = pm.Normal('mu', 0, 2)\n        sigma = pm.HalfNormal('sigma', sigma=1)\n        xs = pm.LogitNormal('xs', mu=mu, sigma=sigma, shape=len(data_ns))\n        ks = pm.Binomial('ks', n=data_ns, p=xs, observed=data_ks)\n    return model \n```", "```py\n%time model = make_model()\npm.model_to_graphviz(model) \n```", "```py\nCPU times: user 875 ms, sys: 51.7 ms, total: 927 ms\nWall time: 2.22 s \n```", "```py\nwith model:\n    pred = pm.sample_prior_predictive(1000)\n    %time trace = pm.sample(500, target_accept=0.97) \n```", "```py\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [xs, sigma, mu] \n```", "```py\nSampling 4 chains for 1_000 tune and 500 draw iterations (4_000 + 2_000 draws total) took 8 seconds.\nThere were 2 divergences after tuning. Increase `target_accept` or reparameterize.\nThe acceptance probability does not match the target. It is 0.9060171753417431, but should be close to 0.97\\. Try to increase the number of tuning steps.\nThere was 1 divergence after tuning. Increase `target_accept` or reparameterize.\nThere were 7 divergences after tuning. Increase `target_accept` or reparameterize.\nThe acceptance probability does not match the target. It is 0.9337619072936738, but should be close to 0.97\\. Try to increase the number of tuning steps.\nThe estimated number of effective samples is smaller than 200 for some parameters. \n```", "```py\nCPU times: user 5.12 s, sys: 153 ms, total: 5.27 s\nWall time: 12.3 s \n```", "```py\nimport arviz as az\n\nwith model:\n    az.plot_posterior(trace, var_names=['mu', 'sigma']) \n```", "```py\ntrace_xs = trace['xs'].transpose()\ntrace_xs.shape \n```", "```py\n(13, 2000) \n```", "```py\nwith model:\n    az.plot_posterior(trace_xs[0]) \n```", "```py\nimport numpy as np\nfrom scipy.stats import norm\n\nmus = np.linspace(-6, 6, 101)\nps = norm.pdf(mus, 0, 2)\nprior_mu = make_pmf(ps, mus, 'mu')\n\nprior_mu.plot()\ndecorate(title='Prior distribution of mu') \n```", "```py\nfrom scipy.stats import logistic\n\nsigmas = np.linspace(0.03, 3.6, 90)\nps = norm.pdf(sigmas, 0, 1)\nprior_sigma = make_pmf(ps, sigmas, 'sigma')\n\nprior_sigma.plot()\ndecorate(title='Prior distribution of sigma') \n```", "```py\ncompare_cdf(prior_mu, pred['mu'])\ndecorate(title='Prior distribution of mu') \n```", "```py\n2.6020852139652106e-18 -0.06372282505953483 \n```", "```py\ncompare_cdf(prior_sigma, pred['sigma'])\ndecorate(title='Prior distribution of sigma') \n```", "```py\n0.8033718951689776 0.8244605687886865 \n```", "```py\ndef make_joint(prior_x, prior_y):\n    X, Y = np.meshgrid(prior_x.ps, prior_y.ps, indexing='ij')\n    hyper = X * Y\n    return hyper \n```", "```py\nprior_hyper = make_joint(prior_mu, prior_sigma)\nprior_hyper.shape \n```", "```py\n(101, 90) \n```", "```py\nimport pandas as pd\nfrom utils import plot_contour\n\nplot_contour(pd.DataFrame(prior_hyper, index=mus, columns=sigmas))\ndecorate(title=\"Joint prior of mu and sigma\") \n```", "```py\nxs = np.linspace(0.01, 0.99, 295) \n```", "```py\nfrom scipy.special import logit\n\nM, S, X = np.meshgrid(mus, sigmas, xs, indexing='ij')\nLO = logit(X)\nLO.sum() \n```", "```py\n-6.440927791118156e-10 \n```", "```py\nfrom scipy.stats import norm\n\n%time normpdf = norm.pdf(LO, M, S)\nnormpdf.sum() \n```", "```py\nCPU times: user 69.6 ms, sys: 16.5 ms, total: 86.1 ms\nWall time: 84.9 ms \n```", "```py\n214125.5678798693 \n```", "```py\n%%time\n\nz = (LO-M) / S\nnormpdf = np.exp(-z**2/2) \n```", "```py\nCPU times: user 26 ms, sys: 10.6 ms, total: 36.6 ms\nWall time: 35.1 ms \n```", "```py\ntotals = normpdf.sum(axis=2)\ntotals.shape \n```", "```py\n(101, 90) \n```", "```py\ndef divide(x, y):\n    out = np.zeros_like(x)\n    return np.divide(x, y, out=out, where=(y!=0)) \n```", "```py\nshape = totals.shape + (1,)\nnormpdf = divide(normpdf, totals.reshape(shape))\nnormpdf.shape \n```", "```py\n(101, 90, 295) \n```", "```py\ndef make_prior(hyper):\n\n    # reshape hyper so we can multiply along axis 0\n    shape = hyper.shape + (1,)\n    prior = normpdf * hyper.reshape(shape)\n\n    return prior \n```", "```py\n%time prior = make_prior(prior_hyper)\nprior.sum() \n```", "```py\nCPU times: user 5.57 ms, sys: 0 ns, total: 5.57 ms\nWall time: 4.87 ms \n```", "```py\n0.999937781278039 \n```", "```py\ndef marginal(joint, axis):\n    axes = [i for i in range(3) if i != axis]\n    return joint.sum(axis=tuple(axes)) \n```", "```py\nprior_mu.plot()\nmarginal_mu = Pmf(marginal(prior, 0), mus)\nmarginal_mu.plot()\ndecorate(title='Checking the marginal distribution of mu') \n```", "```py\nprior_sigma.plot()\nmarginal_sigma = Pmf(marginal(prior, 1), sigmas)\nmarginal_sigma.plot()\ndecorate(title='Checking the marginal distribution of sigma') \n```", "```py\nmarginal_x = Pmf(marginal(prior, 2), xs)\nmarginal_x.plot()\ndecorate(title='Checking the marginal distribution of x',\n         ylim=[0, np.max(marginal_x) * 1.05]) \n```", "```py\npred_xs = pred['xs'].transpose()\npred_xs.shape \n```", "```py\n(13, 1000) \n```", "```py\ncompare_cdf(marginal_x, pred_xs[0])\ndecorate(title='Prior distribution of x') \n```", "```py\n0.49996889063901967 0.4879934000104224 \n```", "```py\ndef get_hyper(joint):\n    return joint.sum(axis=2) \n```", "```py\nhyper = get_hyper(prior) \n```", "```py\nplot_contour(pd.DataFrame(hyper, \n                          index=mus, \n                          columns=sigmas))\ndecorate(title=\"Joint prior of mu and sigma\") \n```", "```py\nfrom scipy.stats import binom\n\ndata_k = data_ks[0]\ndata_n = data_ns[0]\n\nlike_x = binom.pmf(data_k, data_n, xs)\nlike_x.shape \n```", "```py\n(295,) \n```", "```py\nplt.plot(xs, like_x)\ndecorate(title='Likelihood of the data') \n```", "```py\ndef update(prior, data):\n    n, k = data\n    like_x = binom.pmf(k, n, xs)\n    posterior = prior * like_x\n    posterior /= posterior.sum()\n    return posterior \n```", "```py\ndata = data_n, data_k\n%time posterior = update(prior, data) \n```", "```py\nCPU times: user 11.6 ms, sys: 11.9 ms, total: 23.5 ms\nWall time: 7.66 ms \n```", "```py\ndef multiple_updates(prior, ns, ks):\n    for data in zip(ns, ks):\n        print(data)\n        posterior = update(prior, data)\n        hyper = get_hyper(posterior)\n        prior = make_prior(hyper)\n    return posterior \n```", "```py\n%time posterior = multiple_updates(prior, data_ns, data_ks) \n```", "```py\n(129, 4)\n(35, 1)\n(228, 18)\n(84, 7)\n(291, 24)\n(270, 16)\n(46, 6)\n(293, 19)\n(241, 15)\n(105, 13)\n(353, 25)\n(250, 11)\n(41, 4)\nCPU times: user 185 ms, sys: 35.4 ms, total: 220 ms\nWall time: 172 ms \n```", "```py\nmarginal_mu = Pmf(marginal(posterior, 0), mus)\ncompare_cdf(marginal_mu, trace['mu']) \n```", "```py\n-2.6478808810110768 -2.5956645549514694 \n```", "```py\nmarginal_sigma = Pmf(marginal(posterior, 1), sigmas)\ncompare_cdf(marginal_sigma, trace['sigma']) \n```", "```py\n0.19272226451430116 0.18501785022543282 \n```", "```py\nmarginal_x = Pmf(marginal(posterior, 2), xs)\ncompare_cdf(marginal_x, trace_xs[-1]) \n```", "```py\n0.07330826956150183 0.07297933578329886 \n```", "```py\ndef compute_hyper_likelihood(ns, ks):\n    shape = ns.shape + mus.shape + sigmas.shape\n    hyper_likelihood = np.empty(shape)\n\n    for i, data in enumerate(zip(ns, ks)):\n        print(data)\n        n, k = data\n        like_x = binom.pmf(k, n, xs)\n        posterior = normpdf * like_x\n        hyper_likelihood[i] = get_hyper(posterior)\n    return hyper_likelihood \n```", "```py\n%time hyper_likelihood = compute_hyper_likelihood(data_ns, data_ks) \n```", "```py\n(129, 4)\n(35, 1)\n(228, 18)\n(84, 7)\n(291, 24)\n(270, 16)\n(46, 6)\n(293, 19)\n(241, 15)\n(105, 13)\n(353, 25)\n(250, 11)\n(41, 4)\nCPU times: user 82 ms, sys: 55.2 ms, total: 137 ms\nWall time: 75.5 ms \n```", "```py\n%time hyper_likelihood_all = hyper_likelihood.prod(axis=0)\nhyper_likelihood_all.sum() \n```", "```py\nCPU times: user 279 \u00b5s, sys: 0 ns, total: 279 \u00b5s\nWall time: 158 \u00b5s \n```", "```py\n1.685854062633571e-14 \n```", "```py\ni = 3\ndata = data_ns[i], data_ks[i]\ndata \n```", "```py\n(84, 7) \n```", "```py\n%time hyper_i = divide(prior_hyper * hyper_likelihood_all, hyper_likelihood[i])\nhyper_i.sum() \n```", "```py\nCPU times: user 310 \u00b5s, sys: 147 \u00b5s, total: 457 \u00b5s\nWall time: 342 \u00b5s \n```", "```py\n4.3344287278716945e-17 \n```", "```py\nprior_i = make_prior(hyper_i) \n```", "```py\nposterior_i = update(prior_i, data) \n```", "```py\nmarginal_mu = Pmf(marginal(posterior_i, 0), mus)\nmarginal_sigma = Pmf(marginal(posterior_i, 1), sigmas)\nmarginal_x = Pmf(marginal(posterior_i, 2), xs) \n```", "```py\ncompare_cdf(marginal_mu, trace['mu']) \n```", "```py\n-2.647880881011078 -2.5956645549514694 \n```", "```py\ncompare_cdf(marginal_sigma, trace['sigma']) \n```", "```py\n0.19272226451430124 0.18501785022543282 \n```", "```py\ncompare_cdf(marginal_x, trace_xs[i]) \n```", "```py\n0.07245354421667904 0.07224440565018131 \n```", "```py\ndef compute_all_marginals(ns, ks):\n    shape = len(ns), len(xs)\n    marginal_xs = np.zeros(shape)\n    numerator = prior_hyper * hyper_likelihood_all\n\n    for i, data in enumerate(zip(ns, ks)):\n        hyper_i = divide(numerator, hyper_likelihood[i])\n        prior_i = make_prior(hyper_i) \n        posterior_i = update(prior_i, data)\n        marginal_xs[i] = marginal(posterior_i, 2)\n\n    return marginal_xs \n```", "```py\n%time marginal_xs = compute_all_marginals(data_ns, data_ks) \n```", "```py\nCPU times: user 184 ms, sys: 49.8 ms, total: 234 ms\nWall time: 173 ms \n```", "```py\nfor i, ps in enumerate(marginal_xs):\n    pmf = Pmf(ps, xs)\n    plt.figure()\n    compare_cdf(pmf, trace_xs[i])\n    decorate(title=f'Posterior marginal of x for Hospital {i}',\n             xlabel='Death rate',\n             ylabel='CDF',\n             xlim=[trace_xs[i].min(), trace_xs[i].max()]) \n```", "```py\n0.06123636407822421 0.0617519291444324\n0.06653003152551518 0.06643868288267936\n0.07267383211481376 0.07250041300148316\n0.07245354421667904 0.07224440565018131\n0.07430385699796423 0.07433369435815212\n0.06606326919655045 0.06646020352443961\n0.07774639529896528 0.07776805141855801\n0.06788483681522386 0.06807113157490664\n0.06723306224279789 0.06735326167909643\n0.08183332535205982 0.08115900598539395\n0.07003760661997555 0.0704088595242495\n0.06136130741477605 0.06159674913422137\n0.07330826956150185 0.07297933578329886 \n```", "```py\nfor i, ps in enumerate(marginal_xs):\n    pmf = Pmf(ps, xs)\n    diff = abs(pmf.mean() - trace_xs[i].mean()) / pmf.mean()\n    print(diff * 100) \n```", "```py\n0.841926319383687\n0.13730437329010417\n0.23862662568368032\n0.28865194761527047\n0.04015586995533174\n0.6008396688759207\n0.027854821447936134\n0.274427646029194\n0.17878024931315142\n0.8240155997142278\n0.5300765148763152\n0.38369736461746806\n0.44869941709241024 \n```"]