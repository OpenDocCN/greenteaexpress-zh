["```py\nfrom scipy.stats import gamma\n\nalpha = 1.4\nprior_dist = gamma(alpha) \n```", "```py\nimport numpy as np\nfrom utils import pmf_from_dist\n\nlams = np.linspace(0, 10, 101)\nprior_pmf = pmf_from_dist(prior_dist, lams) \n```", "```py\nfrom scipy.stats import poisson\n\ndata = 4\nlikelihood = poisson.pmf(data, lams) \n```", "```py\nposterior = prior_pmf * likelihood\nposterior.normalize() \n```", "```py\n0.05015532557804499 \n```", "```py\nsample_prior = prior_dist.rvs(1000) \n```", "```py\nfrom scipy.stats import poisson\n\nsample_prior_pred = poisson.rvs(sample_prior) \n```", "```py\nfrom empiricaldist import Pmf\n\npmf_prior_pred = Pmf.from_seq(sample_prior_pred) \n```", "```py\nfrom utils import decorate\n\npmf_prior_pred.bar()\ndecorate(xlabel='Number of goals',\n         ylabel='PMF',\n         title='Prior Predictive Distribution') \n```", "```py\nimport pymc3 as pm\n\nwith pm.Model() as model:\n    lam = pm.Gamma('lam', alpha=1.4, beta=1.0)\n    goals = pm.Poisson('goals', lam) \n```", "```py\npm.model_to_graphviz(model) \n```", "```py\nwith model:\n    trace = pm.sample_prior_predictive(1000) \n```", "```py\nsample_prior_pymc = trace['lam']\nsample_prior_pymc.shape \n```", "```py\n(1000,) \n```", "```py\nfrom empiricaldist import Cdf\n\ndef plot_cdf(sample, **options):\n  \"\"\"Plot the CDF of a sample.\n\n sample: sequence of quantities\n \"\"\"\n    Cdf.from_seq(sample).plot(**options) \n```", "```py\nplot_cdf(sample_prior, \n         label='SciPy sample',\n         color='C5')\nplot_cdf(sample_prior_pymc, \n         label='PyMC3 sample',\n         color='C0')\ndecorate(xlabel=r'Goals per game ($\\lambda$)',\n         ylabel='CDF',\n         title='Prior distribution') \n```", "```py\nsample_prior_pred_pymc = trace['goals']\nsample_prior_pred_pymc.shape \n```", "```py\n(1000,) \n```", "```py\ndef plot_pred(sample, **options):\n    Cdf.from_seq(sample).step(**options) \n```", "```py\nplot_pred(sample_prior_pred, \n          label='SciPy sample', \n          color='C5')\nplot_pred(sample_prior_pred_pymc, \n          label='PyMC3 sample', \n          color='C13')\ndecorate(xlabel='Number of goals',\n         ylabel='PMF',\n         title='Prior Predictive Distribution') \n```", "```py\nwith pm.Model() as model:\n    lam = pm.Gamma('lam', alpha=1.4, beta=1.0)\n    goals = pm.Poisson('goals', lam) \n```", "```py\nwith pm.Model() as model2:\n    lam = pm.Gamma('lam', alpha=1.4, beta=1.0)\n    goals = pm.Poisson('goals', lam, observed=4) \n```", "```py\noptions = dict(return_inferencedata=False)\n\nwith model2:\n    trace2 = pm.sample(500, **options) \n```", "```py\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [lam] \n```", "```py\nSampling 2 chains for 1_000 tune and 500 draw iterations (2_000 + 1_000 draws total) took 1 seconds. \n```", "```py\nsample_post_pymc = trace2['lam'] \n```", "```py\nsample_post_pymc.shape \n```", "```py\n(1000,) \n```", "```py\nposterior.make_cdf().plot(label='posterior grid', \n                          color='C5')\nplot_cdf(sample_post_pymc, \n         label='PyMC3 sample',\n         color='C4')\n\ndecorate(xlabel=r'Goals per game ($\\lambda$)',\n         ylabel='CDF',\n         title='Posterior distribution') \n```", "```py\nwith model2:\n    post_pred = pm.sample_posterior_predictive(trace2) \n```", "```py\nsample_post_pred_pymc = post_pred['goals'] \n```", "```py\nsample_post_pred_pymc.shape \n```", "```py\n(1000,) \n```", "```py\nsample_post = posterior.sample(1000)\nsample_post_pred = poisson(sample_post).rvs() \n```", "```py\nplot_pred(sample_post_pred, \n          label='grid sample',\n          color='C5')\nplot_pred(sample_post_pred_pymc, \n          label='PyMC3 sample',\n          color='C12')\n\ndecorate(xlabel='Number of goals',\n         ylabel='PMF',\n         title='Posterior Predictive Distribution') \n```", "```py\nimport pandas as pd\n\nfilename = 'WHR20_DataForFigure2.1.xls'\ndf = pd.read_excel(filename) \n```", "```py\ndf.head(3) \n```", "```py\ndf.shape \n```", "```py\n(153, 20) \n```", "```py\nscore = df['Ladder score'] \n```", "```py\nlog_gdp = df['Logged GDP per capita'] \n```", "```py\nimport matplotlib.pyplot as plt\n\nplt.plot(log_gdp, score, '.')\n\ndecorate(xlabel='Log GDP per capita at PPP',\n         ylabel='Happiness ladder score') \n```", "```py\nfrom scipy.stats import linregress\n\nresult = linregress(log_gdp, score) \n```", "```py\npd.DataFrame([result.slope, result.intercept],\n             index=['Slope', 'Intercept'],\n             columns=['']) \n```", "```py\nx_data = log_gdp\ny_data = score\n\nwith pm.Model() as model3:\n    a = pm.Uniform('a', 0, 4)\n    b = pm.Uniform('b', -4, 4)\n    sigma = pm.Uniform('sigma', 0, 2)\n\n    y_est = a * x_data + b\n    y = pm.Normal('y', \n                  mu=y_est, sd=sigma, \n                  observed=y_data) \n```", "```py\nwith model3:\n    trace3 = pm.sample(500, **options) \n```", "```py\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [sigma, b, a] \n```", "```py\nSampling 2 chains for 1_000 tune and 500 draw iterations (2_000 + 1_000 draws total) took 5 seconds.\nThe number of effective samples is smaller than 25% for some parameters. \n```", "```py\ntrace3 \n```", "```py\n<MultiTrace: 2 chains, 500 iterations, 6 variables> \n```", "```py\nimport arviz as az\n\nwith model3:\n    az.plot_posterior(trace3, var_names=['a', 'b']); \n```", "```py\nprint('Sample mean:', trace3['a'].mean())\nprint('Regression slope:', result.slope) \n```", "```py\nSample mean: 0.715698157714354\nRegression slope: 0.717738495630452 \n```", "```py\nprint('Sample mean:', trace3['b'].mean())\nprint('Regression intercept:', result.intercept) \n```", "```py\nSample mean: -1.174412246262264\nRegression intercept: -1.1986460618088843 \n```", "```py\naz.plot_posterior(trace3['sigma']); \n```", "```py\n20 ** 8 / 1e9 \n```", "```py\n25.6 \n```", "```py\n153 * 20 ** 8 / 1e12 \n```", "```py\n3.9168 \n```", "```py\ncolumns = ['Ladder score',\n           'Logged GDP per capita',\n           'Social support',\n           'Healthy life expectancy',\n           'Freedom to make life choices',\n           'Generosity',\n           'Perceptions of corruption']\n\nsubset = df[columns] \n```", "```py\nsubset.head(3) \n```", "```py\nstandardized = (subset - subset.mean()) / subset.std() \n```", "```py\ny_data = standardized['Ladder score'] \n```", "```py\nx1 = standardized[columns[1]]\nx2 = standardized[columns[2]]\nx3 = standardized[columns[3]]\nx4 = standardized[columns[4]]\nx5 = standardized[columns[5]]\nx6 = standardized[columns[6]] \n```", "```py\nwith pm.Model() as model4:\n    b0 = pm.Uniform('b0', -4, 4)\n    b1 = pm.Uniform('b1', -4, 4)\n    b2 = pm.Uniform('b2', -4, 4)\n    b3 = pm.Uniform('b3', -4, 4)\n    b4 = pm.Uniform('b4', -4, 4)\n    b5 = pm.Uniform('b5', -4, 4)\n    b6 = pm.Uniform('b6', -4, 4)\n    sigma = pm.Uniform('sigma', 0, 2)\n\n    y_est = b0 + b1*x1 + b2*x2 + b3*x3 + b4*x4 + b5*x5 + b6*x6\n    y = pm.Normal('y', \n                  mu=y_est, sd=sigma, \n                  observed=y_data) \n```", "```py\nwith model4:\n    trace4 = pm.sample(500, **options) \n```", "```py\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [sigma, b6, b5, b4, b3, b2, b1, b0] \n```", "```py\nSampling 2 chains for 1_000 tune and 500 draw iterations (2_000 + 1_000 draws total) took 4 seconds. \n```", "```py\ntrace4['b0'].mean() \n```", "```py\n-0.0009400028402880869 \n```", "```py\ntrace4['sigma'].mean() \n```", "```py\n0.5157546237813752 \n```", "```py\nparam_names = ['b1', 'b3', 'b3', 'b4', 'b5', 'b6']\n\nmeans = [trace4[name].mean() \n         for name in param_names] \n```", "```py\ndef credible_interval(sample):\n  \"\"\"Compute 94% credible interval.\"\"\"\n    ci = np.percentile(sample, [3, 97])\n    return np.round(ci, 3)\n\ncis = [credible_interval(trace4[name])\n       for name in param_names] \n```", "```py\nindex = columns[1:]\ntable = pd.DataFrame(index=index)\ntable['Posterior mean'] = np.round(means, 3)\ntable['94% CI'] = cis\ntable \n```", "```py\n# Solution\n\nn = 250\nk_obs = 140\n\nwith pm.Model() as model5:\n    x = pm.Beta('x', alpha=1, beta=1)\n    k = pm.Binomial('k', n=n, p=x, observed=k_obs)\n    trace5 = pm.sample(500, **options)\n    az.plot_posterior(trace5) \n```", "```py\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [x] \n```", "```py\nSampling 2 chains for 1_000 tune and 500 draw iterations (2_000 + 1_000 draws total) took 1 seconds. \n```", "```py\n# Solution\n\nk = 23\nn = 19\nx = 4\n\nwith pm.Model() as model6:\n    N = pm.DiscreteUniform('N', 50, 500)\n    y = pm.HyperGeometric('y', N=N, k=k, n=n, observed=x)\n    trace6 = pm.sample(1000, **options)\n    az.plot_posterior(trace6) \n```", "```py\nMultiprocess sampling (2 chains in 2 jobs)\nMetropolis: [N] \n```", "```py\nSampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 1 seconds.\nThe number of effective samples is smaller than 25% for some parameters. \n```", "```py\ndata = [0.80497283, 2.11577082, 0.43308797, 0.10862644, 5.17334866,\n       3.25745053, 3.05555883, 2.47401062, 0.05340806, 1.08386395] \n```", "```py\n# Solution\n\nwith pm.Model() as model7:\n    lam = pm.Uniform('lam', 0.1, 10.1)\n    k = pm.Uniform('k', 0.1, 5.1)\n    y = pm.Weibull('y', alpha=k, beta=lam, observed=data)\n    trace7 = pm.sample(1000, **options)\n    az.plot_posterior(trace7) \n```", "```py\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [k, lam] \n```", "```py\nSampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 2 seconds.\nThe acceptance probability does not match the target. It is 0.8819724175144361, but should be close to 0.8\\. Try to increase the number of tuning steps. \n```", "```py\ndata = responses['Treated'] \n```", "```py\n# Solution\n\nwith pm.Model() as model8:\n    mu = pm.Uniform('mu', 20, 80)\n    sigma = pm.Uniform('sigma', 5, 30)\n    y = pm.Normal('y', mu, sigma, observed=data)\n    trace8 = pm.sample(500, **options) \n```", "```py\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [sigma, mu] \n```", "```py\nSampling 2 chains for 1_000 tune and 500 draw iterations (2_000 + 1_000 draws total) took 2 seconds. \n```", "```py\n# Solution\n\nwith model8:\n    az.plot_posterior(trace8) \n```", "```py\nk00 = N - num_seen \n```", "```py\ndata = pm.math.stack((k00, k01, k10, k11)) \n```", "```py\nk10 = 20 - 3\nk01 = 15 - 3\nk11 = 3 \n```", "```py\nnum_seen = k01 + k10 + k11\nnum_seen \n```", "```py\n32 \n```", "```py\n# Solution\n\nwith pm.Model() as model9:\n    p0 = pm.Beta('p0', alpha=1, beta=1)\n    p1 = pm.Beta('p1', alpha=1, beta=1)\n    N = pm.DiscreteUniform('N', num_seen, 350)\n\n    q0 = 1-p0\n    q1 = 1-p1\n    ps = [q0*q1, q0*p1, p0*q1, p0*p1]\n\n    k00 = N - num_seen\n    data = pm.math.stack((k00, k01, k10, k11))\n    y = pm.Multinomial('y', n=N, p=ps, observed=data) \n```", "```py\n# Solution\n\nwith model9:\n    trace9 = pm.sample(1000, **options) \n```", "```py\nMultiprocess sampling (2 chains in 2 jobs)\nCompoundStep\n>NUTS: [p1, p0]\n>Metropolis: [N] \n```", "```py\nSampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 3 seconds.\nThe acceptance probability does not match the target. It is 0.5430480274605854, but should be close to 0.8\\. Try to increase the number of tuning steps.\nThe rhat statistic is larger than 1.05 for some parameters. This indicates slight problems during sampling.\nThe estimated number of effective samples is smaller than 200 for some parameters. \n```", "```py\n# Solution\n\nwith model9:\n    az.plot_posterior(trace9) \n```"]