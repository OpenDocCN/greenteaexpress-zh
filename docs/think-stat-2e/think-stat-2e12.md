# 第十一章：回归

> 原文：[`greenteapress.com/thinkstats2/html/thinkstats2012.html`](https://greenteapress.com/thinkstats2/html/thinkstats2012.html)
> 
> 译者：[飞龙](https://github.com/wizardforcel)
> 
> 协议：[CC BY-NC-SA 4.0](http://creativecommons.org/licenses/by-nc-sa/4.0/)


在上一章中的线性最小二乘拟合是回归的一个例子，回归是将任何类型的模型拟合到任何类型的数据的更一般的问题。这种使用术语“回归”的方式是历史偶然事件；它只间接地与该词的原始含义相关。

回归分析的目标是描述一个变量集（称为因变量）与另一个变量集（称为自变量或解释变量）之间的关系。

在上一章中，我们使用母亲的年龄作为解释变量来预测出生体重作为因变量。当只有一个因变量和一个解释变量时，那就是简单回归。在本章中，我们将转向多元回归，有多个解释变量。如果有多个因变量，那就是多元回归。

如果因变量和自变量之间的关系是线性的，那就是线性回归。例如，如果因变量是 y，自变量是 x[1]和 x[2]，我们将写下以下线性回归模型：

```py
y = β[0] + β[1] x[1] + β[2] x[2] + ε
```

其中β[0]是截距，β[1]是与 x[1]相关的参数，β[2]是与 x[2]相关的参数，ε是由于随机变化或其他未知因素而产生的残差。

给定 y 的一系列值和 x[1]和 x[2]的序列，我们可以找到最小化ε²总和的参数β[0]、β[1]和β[2]。这个过程称为普通最小二乘法。计算类似于`thinkstats2.LeastSquare`，但是泛化以处理多个解释变量。您可以在[`en.wikipedia.org/wiki/Ordinary_least_squares`](https://en.wikipedia.org/wiki/Ordinary_least_squares)找到详细信息

本章的代码在`regression.py`中。有关下载和使用此代码的信息，请参阅第 0.2 节。

## 11.1 StatsModels

在上一章中，我介绍了`thinkstats2.LeastSquares`，这是一个用于简单线性回归的实现，旨在易于阅读。对于多元回归，我们将切换到 StatsModels，这是一个提供多种形式的回归和其他分析的 Python 包。如果您使用 Anaconda，您已经拥有 StatsModels；否则，您可能需要安装它。

例如，我将使用 StatsModels 运行上一章的模型：

```py
 import statsmodels.formula.api as smf

    live, firsts, others = first.MakeFrames()
    formula = 'totalwgt_lb ~ agepreg'
    model = smf.ols(formula, data=live)
    results = model.fit() 
```

`statsmodels`提供两种接口（API）；“公式”API 使用字符串来标识因变量和解释变量。它使用一种称为`patsy`的语法；在这个例子中，`~`运算符将左侧的因变量与右侧的解释变量分开。

`smf.ols`接受公式字符串和 DataFrame `live`，并返回一个代表模型的 OLS 对象。`ols`代表“普通最小二乘法”。

`fit`方法将模型拟合到数据并返回一个包含结果的 RegressionResults 对象。

结果也可用作属性。`params`是一个将变量名映射到其参数的 Series，因此我们可以这样获取截距和斜率：

```py
 inter = results.params['Intercept']
    slope = results.params['agepreg'] 
```

估计的参数为 6.83 和 0.0175，与`LeastSquares`相同。

`pvalues`是一个将变量名映射到相关 p 值的 Series，因此我们可以检查估计的斜率是否具有统计显著性：

```py
 slope_pvalue = results.pvalues['agepreg'] 
```

与`agepreg`相关的 p 值为`5.7e-11`，小于 0.001，符合预期。

`results.rsquared`包含 R²，为 0.0047。`results`还提供了`f_pvalue`，它是与整个模型相关的 p 值，类似于测试 R²是否具有统计显著性。

`results`还提供了`resid`，一个残差序列，和`fittedvalues`，与`agepreg`对应的一系列拟合值。

结果对象提供了 `summary()`，以可读的格式表示结果。

```py
 print(results.summary()) 
```

但它打印了很多目前不相关的信息，所以我使用了一个更简单的函数叫做 `SummarizeResults`。这是这个模型的结果：

```py
Intercept       6.83    (0)
agepreg         0.0175  (5.72e-11)
R² 0.004738
Std(ys) 1.408
Std(res) 1.405 
```

`Std(ys)` 是因变量的标准差，如果没有任何解释变量的情况下猜测出生体重，它就是 RMSE。`Std(res)` 是残差的标准差，如果你的猜测受到母亲年龄的影响，它就是 RMSE。正如我们已经看到的，知道母亲的年龄对预测没有实质性的改进。

## 11.2 多元回归

在第 4.5 节中，我们看到第一胎婴儿比其他婴儿更轻，这种影响在统计上是显著的。但这是一个奇怪的结果，因为没有明显的机制会导致第一胎婴儿更轻。所以我们可能会怀疑这种关系是虚假的。

事实上，这种影响有可能有一个解释。我们已经看到出生体重取决于母亲的年龄，我们可能会期望第一胎婴儿的母亲比其他人年轻。

通过一些计算，我们可以检查这种解释是否合理。然后我们将使用多元回归更仔细地调查。首先，让我们看看体重差异有多大：

```py
diff_weight = firsts.totalwgt_lb.mean() - others.totalwgt_lb.mean() 
```

第一胎婴儿比较轻 0.125 磅，或者 2 盎司。年龄的差异：

```py
diff_age = firsts.agepreg.mean() - others.agepreg.mean() 
```

第一胎婴儿的母亲年轻了 3.59 岁。再次运行线性模型，我们得到了出生体重随年龄变化的变化：

```py
results = smf.ols('totalwgt_lb ~ agepreg', data=live).fit()
slope = results.params['agepreg'] 
```

斜率是每年 0.0175 磅。如果我们将斜率乘以年龄差异，我们就得到了母亲年龄导致第一胎婴儿和其他婴儿出生体重差异的预期值：

```py
slope * diff_age 
```

结果是 0.063，正好是观察到的差异的一半。所以我们暂时得出结论，观察到的出生体重差异部分可以通过母亲年龄的差异来解释。

使用多元回归，我们可以更系统地探索这些关系。

```py
 live['isfirst'] = live.birthord == 1
    formula = 'totalwgt_lb ~ isfirst'
    results = smf.ols(formula, data=live).fit() 
```

第一行创建了一个名为 `isfirst` 的新列，对于第一胎婴儿为真，否则为假。然后我们使用 `isfirst` 作为解释变量拟合模型。

这是结果：

```py
Intercept         7.33   (0)
isfirst[T.True]  -0.125  (2.55e-05)
R² 0.00196 
```

因为 `isfirst` 是一个布尔值，`ols` 将其视为一个分类变量，这意味着值分为类别，比如真和假，并且不应该被视为数字。估计的参数是 `isfirst` 为真时对出生体重的影响，所以结果-0.125 磅是第一胎婴儿和其他婴儿出生体重的差异。

斜率和截距在统计上是显著的，这意味着它们不太可能是偶然发生的，但是这个模型的 R²值很小，这意味着 `isfirst` 并不能解释出生体重变化的重要部分。

`agepreg` 的结果类似：

```py
Intercept       6.83    (0)
agepreg         0.0175  (5.72e-11)
R² 0.004738 
```

再次，参数在统计上是显著的，但 R²值很低。

这些模型证实了我们已经看到的结果。但现在我们可以拟合一个包括两个变量的单一模型。使用公式 `totalwgt_lb ~ isfirst + agepreg`，我们得到：

```py
Intercept        6.91    (0)
isfirst[T.True] -0.0698  (0.0253)
agepreg          0.0154  (3.93e-08)
R² 0.005289 
```

在组合模型中，`isfirst` 的参数大约减小了一半，这意味着 `isfirst` 的一部分明显影响实际上是由 `agepreg` 解释的。而 `isfirst` 的 p 值约为 2.5%，处于统计显著性的边缘。

这个模型的 R²值稍微高一些，这表明这两个变量一起解释出生体重的变化比单独解释要多（但并不多）。

## 11.3 非线性关系

记住 `agepreg` 的贡献可能是非线性的，我们可以考虑添加一个变量来捕捉更多的这种关系。一个选择是创建一个名为 `agepreg2` 的列，其中包含年龄的平方：

```py
 live['agepreg2'] = live.agepreg**2
    formula = 'totalwgt_lb ~ isfirst + agepreg + agepreg2' 
```

现在通过估计 `agepreg` 和 `agepreg2` 的参数，我们有效地拟合了一个抛物线：

```py
Intercept        5.69     (1.38e-86)
isfirst[T.True] -0.0504   (0.109)
agepreg          0.112    (3.23e-07)
agepreg2        -0.00185  (8.8e-06)
R² 0.007462 
```

`agepreg2`的参数是负的，因此抛物线向下弯曲，这与图 10.2 中的线条形状一致。

`agepreg`的二次模型解释了出生体重变异的更多部分；在这个模型中，`isfirst`的参数更小，不再具有统计学意义。

像`agepreg2`这样的计算变量是拟合多项式和其他函数到数据的常见方法。这个过程仍然被认为是线性回归，因为因变量是解释变量的线性函数，无论一些变量是否是其他变量的非线性函数。

以下表格总结了这些回归的结果：

|   | isfirst | agepreg | agepreg2 | R² |
| --- | --- | --- | --- | --- |
| 模型 1 | -0.125 * | – | – | 0.002 |
| 模型 2 | – | 0.0175 * | – | 0.0047 |
| 模型 3 | -0.0698 (0.025) | 0.0154 * | – | 0.0053 |
| 模型 4 | -0.0504 (0.11) | 0.112 * | -0.00185 * | 0.0075 |

这个表中的列是解释变量和决定系数 R²。每个条目都是一个估计参数，要么是括号中的 p 值，要么是表示 p 值小于 0.001 的星号。

我们得出结论，出生体重的明显差异至少部分是由母亲年龄的差异解释的。当我们在模型中包括母亲年龄时，`isfirst`的影响变小了，剩下的影响可能是由于偶然。

在这个例子中，母亲的年龄充当控制变量；在模型中包括`agepreg`“控制”了初次产妇和其他人之间的年龄差异，从而有可能分离出`isfirst`的影响（如果有的话）。

## 11.4 数据挖掘

到目前为止，我们已经使用回归模型进行解释；例如，在前一节中，我们发现出生体重的明显差异实际上是由于母亲年龄的差异。但是这些模型的 R²值非常低，这意味着它们的预测能力很小。在本节中，我们将尝试做得更好。

假设你的一个同事正在期待宝宝的出生重量，并且有一个办公室彩池来猜测宝宝的出生重量（如果你不熟悉彩池，请参见[`en.wikipedia.org/wiki/Betting_pool`](https://en.wikipedia.org/wiki/Betting_pool)）。

现在假设你*真的*想赢得彩池。你能做些什么来提高你的机会？嗯，NSFG 数据集包括每个怀孕约 244 个变量，以及每个受访者约 3087 个变量。也许其中一些变量具有预测能力。要找出哪些变量最有用，为什么不尝试它们呢？

测试怀孕表中的变量很容易，但是为了使用受访者表中的变量，我们必须将每个怀孕与受访者匹配起来。理论上，我们可以遍历怀孕表的行，使用`caseid`找到相应的受访者，并将对应表中的值复制到怀孕表中。但那样会很慢。

更好的选择是将这个过程视为 SQL 和其他关系数据库语言中定义的连接操作（参见[`en.wikipedia.org/wiki/Join_(SQL)`](https://en.wikipedia.org/wiki/Join_(SQL))）。连接作为 DataFrame 方法实现，因此我们可以这样执行操作：

```py
 live = live[live.prglngth>30]
    resp = chap01soln.ReadFemResp()
    resp.index = resp.caseid
    join = live.join(resp, on='caseid', rsuffix='_r') 
```

第一行选择了怀孕超过 30 周的记录，假设办公室彩池是在预产期前几周形成的。

下一行读取了受访者文件。结果是一个带有整数索引的 DataFrame；为了高效查找受访者，我用`resp.caseid`替换了`resp.index`。

`join`方法在`live`上调用，被认为是“左”表，并传递`resp`，被认为是“右”表。关键字参数`on`指示用于匹配两个表的行的变量。

在这个例子中，一些列名同时出现在两个表中，因此我们必须提供`rsuffix`，这是一个字符串，将附加到右表中重叠列的名称。例如，两个表都有一个名为`race`的列，用于编码受访者的种族。连接的结果包含两个名为`race`和`race_r`的列。

Pandas 的实现速度很快。在普通台式电脑上，连接 NSFG 表不到一秒钟。现在我们可以开始测试变量了。

```py
 t = []
    for name in join.columns:
        try:
            if join[name].var() < 1e-7:
                continue

            formula = 'totalwgt_lb ~ agepreg + ' + name
            model = smf.ols(formula, data=join)
            if model.nobs < len(join)/2:
                continue

            results = model.fit()
        except (ValueError, TypeError):
            continue

        t.append((results.rsquared, name)) 
```

对于每个变量，我们构建一个模型，计算 R²，并将结果附加到列表中。所有模型都包括`agepreg`，因为我们已经知道它具有一定的预测能力。

我检查每个解释变量是否具有一定的变异性；否则，回归的结果就不可靠。我还检查每个模型的观察数量。包含大量`nan`的变量不适合用于预测。

对于这些变量中的大多数，我们还没有进行任何清理。其中一些以不太适合线性回归的方式进行编码。因此，我们可能会忽略一些如果清理得当就会有用的变量。但也许我们会找到一些好的候选者。

## 11.5 预测

下一步是对结果进行排序，并选择产生最高 R²值的变量。

```py
 t.sort(reverse=True)
    for mse, name in t[:30]:
        print(name, mse) 
```

列表中的第一个变量是`totalwgt_lb`，其次是`birthwgt_lb`。显然，我们不能用出生体重来预测出生体重。

同样，`prglngth`具有有用的预测能力，但是对于办公室彩票，我们假设怀孕长度（以及相关变量）尚未知晓。

第一个有用的预测变量是`babysex`，表示婴儿是男性还是女性。在 NSFG 数据集中，男孩比女孩重约 0.3 磅。因此，假设知道婴儿的性别，我们可以用它进行预测。

接下来是`race`，表示受访者是白人、黑人还是其他。作为解释变量，种族可能存在问题。在像 NSFG 这样的数据集中，种族与许多其他变量相关，包括收入和其他社会经济因素。在回归模型中，种族充当代理变量，因此与种族的表面相关往往至少部分是由其他因素引起的。

列表中的下一个变量是`nbrnaliv`，表示怀孕是否产生了多胞胎。双胞胎和三胞胎往往比其他婴儿小，因此如果我们知道我们假设的同事是否正在期待双胞胎，那将会有所帮助。

列表中的下一个是`paydu`，表示受访者是否拥有自己的住房。这是几个与收入相关的变量之一，结果证明具有预测能力。在像 NSFG 这样的数据集中，收入和财富与几乎所有事物都相关。在这个例子中，收入与饮食、健康、医疗保健和其他可能影响出生体重的因素有关。

列表中的其他一些变量是直到后来才能知道的，比如`bfeedwks`，婴儿哺乳的周数。我们不能用这些变量进行预测，但你可能想推测一下`bfeedwks`可能与出生体重相关的原因。

有时你从理论出发，用数据来测试它。有时你从数据出发，寻找可能的理论。这一节展示的第二种方法称为数据挖掘。数据挖掘的优点是它可以发现意想不到的模式。危险在于它发现的许多模式要么是随机的，要么是虚假的。

在确定了潜在的解释变量之后，我测试了一些模型，并最终选择了这个模型：

```py
 formula = ('totalwgt_lb ~ agepreg + C(race) + babysex==1 + '
               'nbrnaliv>1 + paydu==1 + totincr')
    results = smf.ols(formula, data=join).fit() 
```

这个公式使用了一些我们还没有见过的语法：`C(race)`告诉公式解析器（Patsy）将种族视为分类变量，即使它是以数字编码的。

`babysex`的编码是 1 表示男性，2 表示女性；写成`babysex==1`将其转换为布尔值，男性为 True，女性为 False。

同样，`nbrnaliv>1`对于多胞胎是 True，`paydu==1`对于拥有自己房屋的受访者是 True。

`totincr`从 1-14 进行数字编码，每个增量代表大约 5000 美元的年收入。所以我们可以将这些值视为数值，以 5000 美元为单位。

以下是模型的结果：

```py
Intercept               6.63    (0)
C(race)[T.2]            0.357   (5.43e-29)
C(race)[T.3]            0.266   (2.33e-07)
babysex == 1[T.True]    0.295   (5.39e-29)
nbrnaliv > 1[T.True]   -1.38    (5.1e-37)
paydu == 1[T.True]      0.12    (0.000114)
agepreg                 0.00741 (0.0035)
totincr                 0.0122  (0.00188) 
```

种族的估计参数比我预期的要大，尤其是在我们控制收入的情况下。编码是 1 代表黑人，2 代表白人，3 代表其他。黑人母亲的宝宝比其他种族的宝宝轻 0.27-0.36 磅。

正如我们已经看到的，男孩重大约 0.3 磅；双胞胎和其他多胞胎则轻 1.4 磅。

拥有自己房屋的人，即使在我们控制收入的情况下，他们的宝宝也会重 0.12 磅。母亲年龄的参数比我们在第[11.2]节中看到的要小，这表明其他一些变量与年龄相关，可能包括`paydu`和`totincr`。

所有这些变量都具有统计学意义，有些具有非常低的 p 值，但 R²仅为 0.06，仍然非常小。没有使用模型的 RMSE 为 1.27 磅；使用模型后下降到 1.23。所以你赢得彩票的机会并没有得到实质性的改善。抱歉！

## 11.6 逻辑回归

在前面的例子中，一些解释变量是数值的，一些是分类的（包括布尔型）。但因变量总是数值的。

线性回归可以推广处理其他类型的因变量。如果因变量是布尔型的，推广模型称为逻辑回归。如果因变量是整数计数，它被称为泊松回归。

作为逻辑回归的一个例子，让我们考虑办公室彩票情景的一个变化。假设你的一个朋友怀孕了，你想预测宝宝是男孩还是女孩。你可以使用 NSFG 的数据来找到影响“性别比”，这通常被定义为生男孩的概率的因素。

如果你将因变量进行数字编码，例如 0 代表女孩，1 代表男孩，你可以应用普通最小二乘法，但会有问题。线性模型可能是这样的：

```py
y = β[0] + β[1] x[1] + β[2] x[2] + ε
```

其中 y 是因变量，x[1]和 x[2]是解释变量。然后我们可以找到最小化残差的参数。

这种方法的问题在于它产生了难以解释的预测。给定估计参数和 x[1]和 x[2]的值，模型可能预测 y=0.5，但 y 的唯一有意义的值是 0 和 1。

很容易将这样的结果解释为概率；例如，我们可能会说具有特定 x[1]和 x[2]值的受访者有 50%的生男孩的机会。但这个模型也可能预测 y=1.1 或 y=−0.1，这些不是有效的概率。

逻辑回归通过用赔率而不是概率来表达预测来避免这个问题。如果你不熟悉赔率，“事件的优势赔率”是它发生的概率与它不发生的概率的比率。

所以如果我认为我的团队有 75%的获胜机会，我会说他们的优势是三比一，因为获胜的机会是失败的三倍。

赔率和概率是相同信息的不同表示。给定一个概率，你可以这样计算赔率：

```py
 o = p / (1-p) 
```

给定优势赔率，你可以这样转换成概率：

```py
 p = o / (o+1) 
```

逻辑回归基于以下模型：

```py
标志 = β[0] + β[1] x[1] + β[2] x[2] + ε
```

其中 o 是某种结果的优势赔率；在这个例子中，o 将是生男孩的赔率。

假设我们已经估计了参数β[0]，β[1]和β[2]（我马上会解释如何做到）。并且假设我们已经得到了 x[1]和 x[2]的值。我们可以计算 logo 的预测值，然后转换为概率：

```py
 o = np.exp(log_o)
    p = o / (o+1) 
```

因此，在办公室抽奖的情况下，我们可以计算生男孩的预测概率。但是我们如何估计参数呢？

## 11.7 估计参数

与线性回归不同，逻辑回归没有封闭形式的解，因此通过猜测初始解并进行迭代改进来解决。

通常的目标是找到最大似然估计（MLE），即最大化数据的可能性的参数集。例如，假设我们有以下数据：

```py
>>> y = np.array([0, 1, 0, 1])
>>> x1 = np.array([0, 0, 0, 1])
>>> x2 = np.array([0, 1, 1, 1]) 
```

首先，我们从初始猜测β[0]=−1.5, β[1]=2.8, 和 β[2]=1.1 开始：

```py
>>> beta = [-1.5, 2.8, 1.1] 
```

然后对于每一行，我们可以计算`log_o`：

```py
>>> log_o = beta[0] + beta[1] * x1 + beta[2] * x2
[-1.5 -0.4 -0.4  2.4] 
```

并将对数几率转换为概率：

```py
>>> o = np.exp(log_o)
[  0.223   0.670   0.670  11.02  ]

>>> p = o / (o+1)
[ 0.182  0.401  0.401  0.916 ] 
```

请注意，当`log_o`大于 0 时，`o`大于 1 且`p`大于 0.5。

当`y==1`时，结果的可能性是`p`，当`y==0`时，结果的可能性是`1-p`。例如，如果我们认为男孩的概率是 0.8，结果是男孩，那么可能性是 0.8；如果结果是女孩，可能性是 0.2。我们可以这样计算：

```py
>>> likes = y * p + (1-y) * (1-p)
[ 0.817  0.401  0.598  0.916 ] 
```

数据的整体可能性是`likes`的乘积：

```py
>>> like = np.prod(likes)
0.18 
```

对于这些`beta`值，数据的可能性是 0.18。逻辑回归的目标是找到最大化这种可能性的参数。为了做到这一点，大多数统计软件包使用像牛顿法这样的迭代求解器（参见[`en.wikipedia.org/wiki/Logistic_regression#Model_fitting`](https://en.wikipedia.org/wiki/Logistic_regression#Model_fitting)）。

## 11.8 实施

StatsModels 提供了一种名为`logit`的逻辑回归实现，该实现以从概率到对数几率的转换函数命名。为了演示其用法，我将寻找影响性别比的变量。

再次，我加载 NSFG 数据并选择怀孕超过 30 周的情况：

```py
 live, firsts, others = first.MakeFrames()
    df = live[live.prglngth>30] 
```

`logit`要求因变量是二进制的（而不是布尔值），因此我创建了一个名为`boy`的新列，使用`astype(int)`转换为二进制整数：

```py
 df['boy'] = (df.babysex==1).astype(int) 
```

已经发现影响性别比的因素包括父母的年龄、出生顺序、种族和社会地位。我们可以使用逻辑回归来查看这些影响是否出现在 NSFG 数据中。我将从母亲的年龄开始：

```py
 import statsmodels.formula.api as smf

    model = smf.logit('boy ~ agepreg', data=df)
    results = model.fit()
    SummarizeResults(results) 
```

`logit`接受与`ols`相同的参数，即 Patsy 语法中的公式和 DataFrame。结果是一个代表模型的 Logit 对象。它包含名为`endog`和`exog`的属性，其中包含内生变量（因变量的另一个名称）和外生变量（解释变量的另一个名称）。由于它们是 NumPy 数组，有时将它们转换为 DataFrames 很方便：

```py
 endog = pandas.DataFrame(model.endog, columns=[model.endog_names])
    exog = pandas.DataFrame(model.exog, columns=model.exog_names) 
```

`model.fit`的结果是一个 BinaryResults 对象，它类似于我们从`ols`得到的 RegressionResults 对象。以下是结果的摘要：

```py
Intercept   0.00579   (0.953)
agepreg     0.00105   (0.783)
R² 6.144e-06 
```

`agepreg`的参数是正的，这表明年龄较大的母亲更有可能生男孩，但 p 值为 0.783，这意味着明显的效应很可能是由于偶然因素。

决定系数 R²不适用于逻辑回归，但有几个替代值被用作“伪 R²值”。这些值对于比较模型很有用。例如，这是一个包括几个被认为与性别比相关的因素的模型：

```py
 formula = 'boy ~ agepreg + hpagelb + birthord + C(race)'
    model = smf.logit(formula, data=df)
    results = model.fit() 
```

除了母亲的年龄，该模型还包括父亲的出生年龄（`hpagelb`）、出生顺序（`birthord`）和种族作为分类变量。以下是结果：

```py
Intercept      -0.0301     (0.772)
C(race)[T.2]   -0.0224     (0.66)
C(race)[T.3]   -0.000457   (0.996)
agepreg        -0.00267    (0.629)
hpagelb         0.0047     (0.266)
birthord        0.00501    (0.821)
R² 0.000144 
```

没有估计的参数是统计上显著的。伪 R²值稍高一些，但这可能是由于偶然因素。

## 11.9 准确性

在办公室抽奖的情况下，我们最感兴趣的是模型的准确性：成功预测的数量，与我们根据偶然因素所期望的相比。

在 NSFG 数据中，男孩比女孩多，因此基准策略是每次都猜“男孩”。这种策略的准确性就是男孩的比例：

```py
 actual = endog['boy']
    baseline = actual.mean() 
```

由于“实际”以二进制整数编码，平均值是男孩的比例，为 0.507。

这是我们计算模型准确性的方法：

```py
 predict = (results.predict() >= 0.5)
    true_pos = predict * actual
    true_neg = (1 - predict) * (1 - actual) 
```

`results.predict`返回一个概率的 NumPy 数组，我们将其四舍五入为 0 或 1。乘以`actual`，如果我们预测男孩并且猜对了，就得到 1，否则得到 0。因此，`true_pos`表示“真正例”。

同样，`true_neg`表示我们猜“女孩”并且猜对的情况。准确性是正确猜测的比例：

```py
 acc = (sum(true_pos) + sum(true_neg)) / len(actual) 
```

结果为 0.512，略优于基准值 0.507。但是，你不应该太认真对待这个结果。我们使用相同的数据来构建和测试模型，因此该模型可能对新数据没有预测能力。

尽管如此，让我们使用该模型来预测办公室竞猜。假设你的朋友 35 岁，是白人，她的丈夫 39 岁，他们正在期待他们的第三个孩子：

```py
 columns = ['agepreg', 'hpagelb', 'birthord', 'race']
    new = pandas.DataFrame([[35, 39, 3, 2]], columns=columns)
    y = results.predict(new) 
```

要调用`results.predict`来预测一个新案例，你必须构建一个 DataFrame，其中包含模型中的每个变量的列。在这种情况下的结果是 0.52，所以你应该猜“男孩”。但是如果该模型提高了你赢得竞猜的机会，差异是非常小的。

## 11.10 练习

我对这些练习的解决方案在`chap11soln.ipynb`中。

练习 1 *假设你的一个同事正在期待宝宝的出生，你正在参与一个办公室竞猜宝宝出生日期的活动。假设在怀孕 30 周时下注，你可以使用哪些变量来做出最佳预测？你应该限制自己使用在出生前已知的变量，并且可能会被其他人知道的变量。*练习 2 *特里弗斯-威拉德假说表明，对于许多哺乳动物，性别比取决于“母体状况”；也就是说，像母亲的年龄、体型、健康和社会地位等因素。请参见* [*https://en.wikipedia.org/wiki/Trivers-Willard_hypothesis*](https://en.wikipedia.org/wiki/Trivers-Willard_hypothesis)

*一些研究表明人类中存在这种影响，但结果是参差不齐的。在本章中，我们测试了一些与这些因素相关的变量，但没有发现任何具有统计显著影响性别比的因素。*

*作为练习，使用数据挖掘方法测试怀孕和受访者文件中的其他变量。你能找到任何具有实质影响的因素吗？*

练习 3 *如果你想要预测的数量是计数的，你可以使用泊松回归，在 StatsModels 中用一个名为`poisson`的函数实现。它的工作方式与`ols`和`logit`相同。作为练习，让我们用它来预测一个女性生了多少个孩子；在 NSFG 数据集中，这个变量称为`numbabes`。*

*假设你遇到一个 35 岁的黑人女性，大学毕业，年收入超过 75000 美元。你会预测她有多少个孩子？*

练习 4 *如果你想要预测的数量是分类的，你可以使用多项式 logistic 回归，这在 StatsModels 中用一个名为`mnlogit`的函数实现。作为练习，让我们用它来猜测一个女性是已婚、同居、丧偶、离婚、分居还是未婚；在 NSFG 数据集中，婚姻状况是用一个名为`rmarital`的变量编码的。*

*假设你遇到一个 25 岁的白人女性，高中毕业，年收入约为 45000 美元。她结婚、同居等的概率是多少？*

## 11.11 术语表

+   回归：用于估计适合数据模型的参数的几个相关过程之一。

+   因变量：回归模型中我们想要预测的变量。也被称为内生变量。

+   解释变量：用于预测或解释因变量的变量。也被称为自变量或外生变量。

+   简单回归：只有一个因变量和一个解释变量的回归。

+   多元回归：具有多个解释变量但只有一个因变量的回归。

+   线性回归：基于线性模型的回归。

+   普通最小二乘法：通过最小化残差的平方误差来估计参数的线性回归。

+   虚假关系：两个变量之间的关系是由统计人为因素或因素引起的，而这些因素没有包括在模型中，但与两个变量相关。

+   控制变量：包括在回归中以消除或“控制”虚假关系的变量。

+   代理变量：由于与另一个因素有关系，间接地为回归模型提供信息的变量，因此充当该因素的代理。

+   分类变量：可以具有一组无序值中的一个的变量。

+   连接：使用键将两个 DataFrame 的数据匹配起来的操作。

+   数据挖掘：通过测试大量模型来寻找变量之间关系的方法。

+   逻辑回归：当因变量为布尔值时使用的一种回归形式。

+   泊松回归：当因变量是非负整数时使用的一种回归形式，通常是计数。

+   赔率：表示概率 p 的另一种方式，即概率及其补数的比率，p / (1−p)。
