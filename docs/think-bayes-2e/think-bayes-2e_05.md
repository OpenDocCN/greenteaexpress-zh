# 第二章：贝叶斯定理

> 原文：[`allendowney.github.io/ThinkBayes2/chap02.html`](https://allendowney.github.io/ThinkBayes2/chap02.html)
> 
> 译者：[飞龙](https://github.com/wizardforcel)
> 
> 协议：[CC BY-NC-SA 4.0](http://creativecommons.org/licenses/by-nc-sa/4.0/)


在上一章中，我们推导了贝叶斯定理：

$$P(A|B) = \frac{P(A) P(B|A)}{P(B)}$$

例如，我们使用了来自普查总体社会调查的数据和贝叶斯定理来计算条件概率。但由于我们有完整的数据集，我们实际上并不需要贝叶斯定理。直接计算等式的左侧足够容易，计算右侧也不容易。

但通常我们没有完整的数据集，在这种情况下，贝叶斯定理更有用。在本章中，我们将使用它来解决与条件概率相关的几个更具挑战性的问题。

## 饼干问题

我们将从一个轻微伪装的[urn problem](https://en.wikipedia.org/wiki/Urn_problem)开始：

> 假设有两个碗装着饼干。
> 
> +   碗 1 中有 30 个香草饼干和 10 个巧克力饼干。
> +   
> +   碗 2 中有 20 个香草饼干和 20 个巧克力饼干。
> +   
> 现在假设你随机选择一个碗，并且在不看的情况下随机选择一个饼干。如果饼干是香草味的，那么它来自碗 1 的概率是多少？

我们想要的是在我们得到一个香草饼干的情况下，我们从碗 1 中选择的条件概率，$P(B_1 | V)$。

但是我们从问题的陈述中得到的是：

+   得到香草饼干的条件概率，假设我们从碗 1 中选择，$P(V | B_1)$和

+   得到香草饼干的条件概率，假设我们从碗 2 中选择，$P(V | B_2)$。

贝叶斯定理告诉我们它们是如何相关的：

$$P(B_1|V) = \frac{P(B_1)~P(V|B_1)}{P(V)}$$

左边的项是我们想要的。右边的项是：

+   $P(B_1)$，我们选择碗 1 的概率，不受我们得到什么样的饼干的影响。由于问题说我们随机选择了一个碗，我们假设$P(B_1) = 1/2$。

+   $P(V|B_1)$，从碗 1 中取出香草饼干的概率，为 3/4。

+   $P(V)$，从任一碗中取出香草饼干的概率。

要计算$P(V)$，我们可以使用总概率定律：

$$P(V) = P(B_1)~P(V|B_1) ~+~ P(B_2)~P(V|B_2)$$

从问题陈述中的数字中插入，我们有

$$P(V) = (1/2)~(3/4) ~+~ (1/2)~(1/2) = 5/8$$

我们也可以直接计算这个结果，如下所示：

+   由于我们有同样的机会选择任何一个碗，并且碗中包含相同数量的饼干，我们选择任何一块饼干的机会是一样的。

+   两个碗之间有 50 个香草饼干和 30 个巧克力饼干，所以$P(V) = 5/8$。

最后，我们可以应用贝叶斯定理来计算碗 1 的后验概率：

$$P(B_1|V) = (1/2)~(3/4)~/~(5/8) = 3/5$$

这个例子演示了贝叶斯定理的一个用途：它提供了一种从$P(B|A)$到$P(A|B)$的方法。这种策略在这样的情况下很有用，其中计算右侧的项比计算左侧的项更容易。

## 历时贝叶斯

还有另一种思考贝叶斯定理的方法：它给了我们一种更新假设$H$的概率的方法，给定一些数据$D$。

这种解释是“历时的”，意思是“与时间变化有关”；在这种情况下，随着我们看到新数据，假设的概率会发生变化。

用$H$和$D$重写贝叶斯定理得到：

$$P(H|D) = \frac{P(H)~P(D|H)}{P(D)}$$

在这种解释中，每个术语都有一个名称：

+   $P(H)$是我们看到数据之前的假设概率，称为先验概率，或者**先验**。

+   $P(H|D)$是我们看到数据后的假设概率，称为**后验**。

+   $P(D|H)$是假设下数据的概率，称为**似然**。

+   $P(D)$是任何假设下数据的**总概率**。

有时我们可以根据背景信息计算先验。例如，饼干问题规定我们以相等的概率随机选择一个碗。

在其他情况下，先验是主观的；也就是说，理性的人可能会有不同意见，要么是因为他们使用了不同的背景信息，要么是因为他们对相同的信息有不同的解释。

似然性通常是最容易计算的部分。在饼干问题中，我们已经知道了每个碗中饼干的数量，因此我们可以计算每个假设下数据的概率。

计算数据的总概率可能会很棘手。它应该是在任何假设下看到数据的概率，但很难确定这意味着什么。

通常我们通过指定一组假设来简化事情，这些假设是：

+   互斥的，这意味着它们中只有一个可以为真，以及

+   集体穷尽的，这意味着其中一个必须为真。

当这些条件适用时，我们可以使用全概率法则计算$P(D)$。例如，对于两个假设$H_1$和$H_2$：

$$P(D) = P(H_1)~P(D|H_1) + P(H_2)~P(D|H_2)$$

以及更一般地，对于任意数量的假设：

$$P(D) = \sum_i P(H_i)~P(D|H_i)$$

在本节中，使用数据和先验概率计算后验概率的过程称为**贝叶斯更新**。

## 贝叶斯表

进行贝叶斯更新的一个方便工具是贝叶斯表。您可以在纸上编写贝叶斯表，也可以使用电子表格，但在本节中我将使用 Pandas 的`DataFrame`。

首先，我将创建一个空的`DataFrame`，每个假设占一行：

```py
import pandas as pd

table = pd.DataFrame(index=['Bowl 1', 'Bowl 2']) 
```

现在我将添加一列来表示先验：

```py
table['prior'] = 1/2, 1/2
table 
```

|  | 先验 |
| --- | --- |
| 碗 1 | 0.5 |
| 碗 2 | 0.5 |

以及一个列来表示似然性：

```py
table['likelihood'] = 3/4, 1/2
table 
```

|  | 先验 | 似然性 |
| --- | --- | --- |
| 碗 1 | 0.5 | 0.75 |
| 碗 2 | 0.5 | 0.50 |

在这里，我们看到与以前的方法的不同之处：我们为两个假设计算了似然性，而不仅仅是碗 1：

+   从碗 1 中得到香草饼干的机会是 3/4。

+   从碗 2 中得到香草饼干的机会是 1/2。

您可能会注意到，似然性不会相加为 1。没关系；每个似然性都是在不同假设条件下的概率。它们不应该相加为 1，如果不相加也没有问题。

下一步与我们使用贝叶斯定理时类似；我们将先验乘以似然性：

```py
table['unnorm'] = table['prior'] * table['likelihood']
table 
```

|  | 先验 | 似然性 | 未归一化 |
| --- | --- | --- | --- |
| 碗 1 | 0.5 | 0.75 | 0.375 |
| 碗 2 | 0.5 | 0.50 | 0.250 |

我称结果为`unnorm`，因为这些值是“未归一化的后验概率”。它们每个都是先验和似然性的乘积：

$$P(H_i)~P(D|H_i)$$

这是贝叶斯定理的分子。如果我们将它们相加，就有

$$P(H_1)~P(D|H_1) + P(H_2)~P(D|H_2)$$

这是贝叶斯定理的分母，$P(D)$。

因此，我们可以这样计算数据的总概率：

```py
prob_data = table['unnorm'].sum()
prob_data 
```

```py
0.625 
```

请注意，我们得到了 5/8，这是我们直接计算$P(D)$得到的结果。

我们可以这样计算后验概率：

```py
table['posterior'] = table['unnorm'] / prob_data
table 
```

|  | 先验 | 似然性 | 未归一化 | 后验 |
| --- | --- | --- | --- | --- |
| 碗 1 | 0.5 | 0.75 | 0.375 | 0.6 |
| 碗 2 | 0.5 | 0.50 | 0.250 | 0.4 |

碗 1 的后验概率为 0.6，这是我们明确使用贝叶斯定理得到的结果。作为奖励，我们还得到了碗 2 的后验概率，为 0.4。

当我们将未归一化的后验相加并进行除法运算时，我们强制后验相加为 1。这个过程称为“归一化”，这就是为什么数据的总概率也被称为“归一化常数”。

## 骰子问题

贝叶斯表也可以解决超过两个假设的问题。例如：

> 假设我有一个盒子，里面有一个 6 面骰子，一个 8 面骰子和一个 12 面骰子。我随机选择一个骰子，掷出 1，那么我选择了 6 面骰子的概率是多少？

在这个例子中，有三个假设具有相等的先验概率。数据是我报告结果是 1。

如果我选择了 6 面骰子，数据的概率是 1/6。如果我选择了 8 面骰子，概率是 1/8，如果我选择了 12 面骰子，概率是 1/12。

这是一个使用整数表示假设的贝叶斯表格：

```py
table2 = pd.DataFrame(index=[6, 8, 12]) 
```

我将使用分数来表示先验概率和似然。这样它们就不会被四舍五入成浮点数。

```py
from fractions import Fraction

table2['prior'] = Fraction(1, 3)
table2['likelihood'] = Fraction(1, 6), Fraction(1, 8), Fraction(1, 12)
table2 
```

|  | 先验 | 似然 |
| --- | --- | --- |
| 6 | 1/3 | 1/6 |
| 8 | 1/3 | 1/8 |
| 12 | 1/3 | 1/12 |

一旦你有了先验和似然，剩下的步骤总是一样的，所以我会把它们放在一个函数中：

```py
def update(table):
  """Compute the posterior probabilities."""
    table['unnorm'] = table['prior'] * table['likelihood']
    prob_data = table['unnorm'].sum()
    table['posterior'] = table['unnorm'] / prob_data
    return prob_data 
```

然后像这样调用它。

```py
prob_data = update(table2) 
```

这是最终的贝叶斯表格：

```py
table2 
```

|  | 先验 | 似然 | 非归一化 | 后验 |
| --- | --- | --- | --- | --- |
| 6 | 1/3 | 1/6 | 1/18 | 4/9 |
| 8 | 1/3 | 1/8 | 1/24 | 1/3 |
| 12 | 1/3 | 1/12 | 1/36 | 2/9 |

6 面骰子的后验概率是 4/9，比其他骰子的概率略高，分别是 3/9 和 2/9。直观上，6 面骰子是最有可能的，因为它产生我们看到的结果的可能性最高。

## 蒙提霍尔问题

接下来我们将使用一个贝叶斯表格来解决概率中最有争议的问题之一。

蒙提霍尔问题是基于一个名为“让我们做个交易”的游戏节目。如果你是节目的参赛者，游戏规则如下：

+   主持人蒙提·霍尔向你展示了三扇关闭的门——编号为 1、2 和 3——并告诉你每扇门后面都有一个奖品。

+   一个奖品很有价值（传统上是一辆汽车），另外两个奖品价值较低（传统上是山羊）。

+   游戏的目的是猜测哪扇门后面有汽车。如果你猜对了，你就可以保留汽车。

假设你选择了门 1。在打开你选择的门之前，蒙提打开了门 3 并揭示了一只山羊。然后蒙提给你选择坚持原来的选择或者换到剩下的未打开的门的选择。

为了最大化赢得汽车的机会，你应该坚持选择门 1 还是换到门 2？

要回答这个问题，我们必须对主持人的行为做一些假设：

1.  蒙提总是打开一扇门，并给你换门的选择。

1.  他从不打开你选择的门或者有汽车的门。

1.  如果你选择了有汽车的门，他会随机选择另一扇门。

在这些假设下，你最好是换门。如果你坚持，你会赢得 1/3 的时间。如果你换，你会赢得 2/3 的时间。

如果你以前没有遇到过这个问题，你可能会发现答案令人惊讶。你不会是唯一一个；许多人都有这样的直觉，即坚持还是换都无所谓。他们推理说，还剩下两扇门，所以汽车在 A 门后面的机会是 50%。但这是错误的。

为了理解这一点，使用贝叶斯表格可能会有所帮助。我们从三个假设开始：汽车可能在门 1、2 或 3 后面。根据问题的陈述，每扇门的先验概率是 1/3。

```py
table3 = pd.DataFrame(index=['Door 1', 'Door 2', 'Door 3'])
table3['prior'] = Fraction(1, 3)
table3 
```

|  | 先验 |
| --- | --- |
| 门 1 | 1/3 |
| 门 2 | 1/3 |
| 门 3 | 1/3 |

数据是蒙提打开了门 3 并揭示了一只山羊。所以让我们考虑每个假设下数据的概率：

+   如果汽车在门 1 后面，蒙提会随机选择门 2 或 3，所以他打开门 3 的概率是 1/2。

+   如果汽车在门 2 后面，蒙提必须打开门 3，所以在这个假设下数据的概率是 1。

+   如果汽车在 3 号门后面，蒙提就不会打开它，所以在这个假设下数据的概率是 0。

以下是似然。

```py
table3['likelihood'] = Fraction(1, 2), 1, 0
table3 
```

|  | 先验 | 似然 |
| --- | --- | --- |
| 门 1 | 1/3 | 1/2 |
| 门 2 | 1/3 | 1 |
| 门 3 | 1/3 | 0 |

现在我们有了先验和似然，我们可以使用`update`来计算后验概率。

```py
update(table3)
table3 
```

|  | 先验 | 似然 | 非归一化 | 后验 |
| --- | --- | --- | --- | --- |
| 门 1 | 1/3 | 1/2 | 1/6 | 1/3 |
| 门 2 | 1/3 | 1 | 1/3 | 2/3 |
| 门 3 | 1/3 | 0 | 0 | 0 |

蒙蒂打开门 3 后，门 1 的后验概率是$1/3$；门 2 的后验概率是$2/3$。所以你最好从门 1 换到门 2。

正如这个例子所示，我们对概率的直觉并不总是可靠的。贝叶斯定理可以通过提供分而治之的策略来帮助：

1.  首先，写下假设和数据。

1.  接下来，找出先验概率。

1.  最后，计算每个假设下数据的可能性。

贝叶斯表格会完成剩下的工作。

## 总结

在本章中，我们使用贝叶斯定理明确解决了饼干问题，并使用了贝叶斯表格。这些方法之间没有真正的区别，但是对于具有两个以上假设的问题，贝叶斯表格可以更容易地计算数据的总概率。

然后我们解决了骰子问题，我们将在下一章再次看到，以及蒙蒂霍尔问题，你可能希望永远不再见到。

如果蒙蒂霍尔问题让你头疼，你并不孤单。但我认为它展示了贝叶斯定理作为解决棘手问题的分而治之策略的力量。我希望它能够解释答案为什么是这样的一些见解。

当蒙蒂打开一扇门时，他提供了我们可以用来更新对汽车位置的信念的信息。部分信息是显而易见的。如果他打开门 3，我们知道汽车不在门 3 后面。但部分信息更微妙。如果汽车在门 2 后面，打开门 3 更有可能，如果汽车在门 1 后面，打开门 3 更不可能。因此，数据支持门 2。我们将在未来章节中回到这个证据的概念。

在下一章中，我们将扩展饼干问题和骰子问题，并从基本概率迈向贝叶斯统计的下一步。

但首先，你可能想要做练习。

## 练习

**练习：**假设你在盒子里有两枚硬币。一枚是普通硬币，一面是正面，一面是反面，另一枚是特殊硬币，两面都是正面。你随机选择一枚硬币，看到一面是正面。你选择了特殊硬币的概率是多少？



```py
# Solution

table4 = pd.DataFrame(index=['Normal', 'Trick'])
table4['prior'] = 1/2
table4['likelihood'] = 1/2, 1

update(table4)
table4 
```

|  | 先验 | 似然 | 未归一化 | 后验 |
| --- | --- | --- | --- | --- |
| 普通 | 0.5 | 0.5 | 0.25 | 0.333333 |

| 技巧 | 0.5 | 1.0 | 0.50 | 0.666667 |

**练习：**假设你遇到一个人，得知他们有两个孩子。你问其中一个孩子是女孩的概率，他们说是。那么两个孩子都是女孩的概率是多少？

提示：从四个同样可能的假设开始。



```py
# Solution

table5 = pd.DataFrame(index=['GG', 'GB', 'BG', 'BB'])
table5['prior'] = 1/4
table5['likelihood'] = 1, 1, 1, 0

update(table5)
table5 
```

|  | 先验 | 似然 | 未归一化 | 后验 |
| --- | --- | --- | --- | --- |
| GG | 0.25 | 1 | 0.25 | 0.333333 |
| GB | 0.25 | 1 | 0.25 | 0.333333 |
| BG | 0.25 | 1 | 0.25 | 0.333333 |

| BB | 0.25 | 0 | 0.00 | 0.000000 |

**练习：**有许多变种的[蒙蒂霍尔问题](https://en.wikipedia.org/wiki/Monty_Hall_problem)。

例如，假设蒙蒂总是在可以的情况下选择门 2，只有在必须时才选择门 3（因为汽车在门 2 后面）。

如果你选择门 1，蒙蒂打开门 2，汽车在门 3 的概率是多少？

如果你选择门 1，蒙蒂打开门 3，汽车在门 2 的概率是多少？



```py
# Solution

# If the car is behind Door 1, Monty would always open Door 2 
# If the car was behind Door 2, Monty would have opened Door 3
# If the car is behind Door 3, Monty would always open Door 2

table6 = pd.DataFrame(index=['Door 1', 'Door 2', 'Door 3'])
table6['prior'] = 1/3
table6['likelihood'] = 1, 0, 1

update(table6)
table6 
```

|  | 先验 | 似然 | 未归一化 | 后验 |
| --- | --- | --- | --- | --- |
| 门 1 | 0.333333 | 1 | 0.333333 | 0.5 |
| 门 2 | 0.333333 | 0 | 0.000000 | 0.0 |

| 门 3 | 0.333333 | 1 | 0.333333 | 0.5 | 

```py
# Solution

# If the car is behind Door 1, Monty would have opened Door 2
# If the car is behind Door 2, Monty would always open Door 3
# If the car is behind Door 3, Monty would have opened Door 2

table7 = pd.DataFrame(index=['Door 1', 'Door 2', 'Door 3'])
table7['prior'] = 1/3
table7['likelihood'] = 0, 1, 0

update(table7)
table7 
```

|  | 先验 | 似然 | 未归一化 | 后验 |
| --- | --- | --- | --- | --- |
| 门 1 | 0.333333 | 0 | 0.000000 | 0.0 |
| 门 2 | 0.333333 | 1 | 0.333333 | 1.0 |

| 门 3 | 0.333333 | 0 | 0.000000 | 0.0 |

**练习：** M&M 巧克力豆是一种小巧的糖衣巧克力，有各种颜色。

制造 M&M 巧克力豆的 Mars 公司不时地改变颜色的混合比例。1995 年，他们推出了蓝色 M&M 巧克力豆。

+   1994 年，一袋普通 M&M 巧克力豆的颜色混合比例为 30%棕色，20%黄色，20%红色，10%绿色，10%橙色，10%米色。

+   1996 年，蓝色占 24%，绿色占 20%，橙色占 16%，黄色占 14%，红色占 13%，棕色占 13%。

假设我的一个朋友有两袋 M&M 巧克力豆，他告诉我一袋是 1994 年的，一袋是 1996 年的。他不告诉我哪一袋是哪一袋，但他给我每袋各给了一个 M&M。一个是黄色的，一个是绿色的。黄色的来自 1994 年的袋子的概率是多少？ 

提示：这个问题的诀窍是要仔细定义假设和数据。



```py
# Solution

# Hypotheses:
# A: yellow from 94, green from 96
# B: yellow from 96, green from 94

table8 = pd.DataFrame(index=['A', 'B'])
table8['prior'] = 1/2
table8['likelihood'] = 0.2*0.2, 0.14*0.1

update(table8)
table8 
```

|  | 先验 | 似然 | 未归一化 | 后验 |
| --- | --- | --- | --- | --- |
| A | 0.5 | 0.040 | 0.020 | 0.740741 |

| B | 0.5 | 0.014 | 0.007 | 0.259259 |
